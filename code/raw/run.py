# =============================================================================
# è®­ç»ƒä¸»ç¨‹åº---æ ‡å‡†ç‰ˆ
# ======================================================================================
# åŠŸèƒ½:é¢å‘ BERT-CLS åˆ†ç±»æµç¨‹çš„è®­ç»ƒè„šæœ¬ï¼Œç”¨ HuggingFace æ ‡å‡†åºåˆ—åˆ†ç±»å¤´åš ABSA è®­ç»ƒ/éªŒè¯/æµ‹è¯•
# ======================================================================================

import logging   
import argparse  
import os 
import sys  
import random   #å¯¼å…¥éšæœºæ•°æ¨¡å—ï¼Œç”¨äºéšæœºæ“ä½œ
import numpy  
from transformers import AdamW    #ä»transformersåº“å¯¼å…¥AdamWä¼˜åŒ–å™¨
from torch.utils.data.sampler import  WeightedRandomSampler   #å¯¼å…¥åŠ æƒéšæœºé‡‡æ ·å™¨ï¼Œç”¨äºä¸å‡è¡¡æ•°æ®é‡‡æ ·
import torch   
from torch.utils.data import DataLoader, random_split, TensorDataset   #å¯¼å…¥æ•°æ®åŠ è½½ç›¸å…³ç±»
import torch.nn.functional as F   #å¯¼å…¥pytorchçš„å‡½æ•°å¼æ¥å£ï¼Œç”¨äºæ¿€æ´»å‡½æ•°ç­‰æ“ä½œ
import  numpy as np 
import copy   
from  tqdm import tqdm   
from transformers import AutoTokenizer, AutoModel   #ä»transformersåº“å¯¼å…¥è‡ªåŠ¨åˆ†è¯å™¨å’Œè‡ªåŠ¨æ¨¡å‹
import torch.nn as nn   #å¯¼å…¥pytorchçš„ç¥ç»ç½‘ç»œæ¨¡å—

from evaluation import *   #ä»evaluationæ¨¡å—å¯¼å…¥æ‰€æœ‰å‡½æ•°ï¼Œç”¨äºæ¨¡å‹è¯„ä¼°
from data_utils import  ABSADataset_absa_bert_semeval_json, ABSADataset_absa_bert_sentihood_json   #ä»data_utilsæ¨¡å—å¯¼å…¥æ•°æ®é›†ç±»
from MyModel import  BERT_ASC_vanila   #ä»MyModelæ¨¡å—å¯¼å…¥BERT_ASC_vanilaæ¨¡å‹ç±»


# ------------------- logger --------------------------- 
logger = logging.getLogger()   #é…ç½®æ—¥å¿—è®°å½•å™¨
logger.setLevel(logging.INFO)  #è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºINFO
logger.addHandler(logging.StreamHandler(sys.stdout))    #å‘æ ‡å‡†è¾“å‡ºæ·»åŠ æ—¥å¿—å¤„ç†å™¨


class Instructor:   
    def __init__(self, opt): 
        self.opt = opt   #ä¿å­˜é…ç½®é€‰é¡¹
        self.model = BERT_ASC_vanila(opt)   #åˆå§‹åŒ–BERT_ASC_vanilaæ¨¡å‹
        self.model.to(self.opt.device)  #å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
        tokenizer = AutoTokenizer.from_pretrained(opt.pretrained_bert_name)   #åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨
        
        if self.opt.dataset=='semeval':   
            #åŠ è½½semevalæ•°æ®é›†çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            self.trainset = ABSADataset_absa_bert_semeval_json(opt.dataset_file['train'], tokenizer, opt)  
            self.testset = ABSADataset_absa_bert_semeval_json(opt.dataset_file['test'], tokenizer, opt)
            assert 0 <= opt.valset_ratio < 1   #ç¡®ä¿éªŒè¯é›†æ¯”ä¾‹åœ¨åˆç†èŒƒå›´å†…
            if opt.valset_ratio > 0:  #å¦‚æœéœ€è¦åˆ’åˆ†éªŒè¯é›†
                valset_len = int(len(self.trainset) * opt.valset_ratio)   #è®¡ç®—éªŒè¯é›†é•¿åº¦
                self.trainset, self.valset = random_split(self.trainset, (len(self.trainset) - valset_len, valset_len))  #åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
            else:    #å¦‚æœä¸éœ€è¦åˆ’åˆ†éªŒè¯é›†ï¼Œä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºéªŒè¯é›†
                self.valset = self.testset 
        else:  
            #åŠ è½½sentihoodæ•°æ®é›†çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            self.trainset = ABSADataset_absa_bert_sentihood_json(opt.dataset_file['train'], tokenizer, opt)  
            self.testset = ABSADataset_absa_bert_sentihood_json(opt.dataset_file['test'], tokenizer, opt)
            self.valset =self.testset   #ä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºè®­ç»ƒé›†
        logger.info('train {0}: dev {1}: test {2}'.format(len(self.trainset), len(self.valset), len(self.testset)))  
        
        if opt.device.type == 'cuda':    #å¦‚æœæ˜¯ä½¿ç”¨GPU
            logger.info('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=opt.device.index)))   #è®°å½•GPUçš„å†…å­˜åˆ†é…æƒ…å†µ

            
    #å®šä¹‰å­¦ä¹ ç‡é¢„çƒ­å‡½æ•°
    def warmup_linear(self, x, warmup=0.002):  
        if x < warmup:  #å¦‚æœåœ¨é¢„çƒ­é˜¶æ®µ
            return x / warmup  #çº¿æ€§å¢åŠ å­¦ä¹ ç‡
        return 1.0 - x   #é¢„çƒ­åçº¿æ€§è¡°å‡å­¦ä¹ ç‡


        
    #-------------------------------------- è®­ç»ƒæ–¹æ³•  ------------------------------------
    def _train(self, optimizer,criterion, train_data_loader, val_data_loader, t_total):   
        max_val_f1 = 0   #è®°å½•éªŒè¯é›†æœ€ä½³æ€§èƒ½æŒ‡æ ‡ï¼ˆæ ¹æ®æ•°æ®é›†ç±»å‹åŠ¨æ€ç¡®å®šï¼‰
        global_step = 0   #å…¨å±€æ­¥æ•°è®¡æ•°å™¨
        best_model_state = None     #å­˜å‚¨æœ€ä½³æ¨¡å‹çš„å‚æ•°å­—å…¸
        best_epoch = -1     #è®°å½•æœ€ä¼˜è½®æ¬¡
        best_model_path = None    #ä¿å­˜è·¯å¾„  
        
        #å¾ªç¯è®­ç»ƒè½®æ¬¡
        for epoch in range(self.opt.num_epoch):  
            loss_total = 0
            setp_total = 0
            logger.info('>' * 100)     #æ‰“å°åˆ†å‰²çº¿
            logger.info('epoch: {}'.format(epoch))   #æ‰“å°å½“å‰è½®æ¬¡
            self.model.train()     #å°†æ¨¡å¼è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            # ------------------- éå†è®­ç»ƒæ•°æ® ---------------
            for i_batch, sample_batched in enumerate(tqdm(train_data_loader)):   
                optimizer.zero_grad()   #æ¢¯åº¦æ¸…é›¶
                sample_batched= [b.to(self.opt.device) for b in sample_batched]    #å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                input_ids, token_type_ids, attention_mask, labels= sample_batched    #è§£åŒ…æ‰¹æ¬¡æ•°æ®
                outputs= self.model(input_ids, token_type_ids, attention_mask, labels)   #æ¨¡å‹å‘å‰ä¼ æ’­ï¼Œå¾—åˆ°è¾“å‡º
                loss = criterion(outputs, labels)   #è®¡ç®—æŸå¤±
                loss.sum().backward()    #åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
                
                #ç´¯åŠ æŸå¤±å’Œæ ·æœ¬æ•°ï¼ˆæ— æ¢¯åº¦è®¡ç®—ï¼‰
                with torch.no_grad():
                    loss_total+= loss.item()
                    setp_total+=len(labels)
                    
                #è®¡ç®—å½“å‰æ­¥éª¤çš„å­¦ä¹ ç‡
                lr_this_step = self.opt.learning_rate * self.warmup_linear(global_step / t_total, self.opt.warmup_proportion) 
                #æ›´æ–°ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
                for param_group in optimizer.param_groups:
                    param_group['lr'] = lr_this_step
                optimizer.step()  #ä¼˜åŒ–å™¨æ›´æ–°å‚æ•°
                global_step += 1  #å…¨å±€æ­¥æ•°åŠ 1


                
            # -------------------------- éªŒè¯é›†ä¸Šè¯„ä¼° ---------------------
            y_true, y_pred, score = self._evaluate_acc_f1(val_data_loader)   #åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå¾—åˆ°çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾å’Œåˆ†æ•°
            if self.opt.dataset  == 'semeval':  
                #è®¡ç®—semevalæ•°æ®é›†çš„Pã€Rã€Fåˆ†æ•°å’Œä¸åŒç±»åˆ«çš„2ç±»ã€3ç±»ã€4ç±»å‡†ç¡®ç‡
                aspect_P, aspect_R, aspect_F = semeval_PRF(y_true, y_pred)  
                sentiment_Acc_4_classes = semeval_Acc(y_true, y_pred, score, 4)
                sentiment_Acc_3_classes = semeval_Acc(y_true, y_pred, score, 3)
                sentiment_Acc_2_classes = semeval_Acc(y_true, y_pred, score, 2)
                max_per= sentiment_Acc_4_classes    #ä»¥4ç±»æƒ…æ„Ÿå‡†ç¡®ç‡ä½œä¸ºæœ€ä½³æŒ‡æ ‡
            else: 
                #è®¡ç®—sentihoodæ•°æ®é›†çš„strict_Accï¼ˆä¸¥æ ¼å‡†ç¡®ç‡ï¼‰ã€Macro_F1ã€Macro_AUC
                aspect_strict_Acc = sentihood_strict_acc(y_true, y_pred) 
                aspect_Macro_F1 = sentihood_macro_F1(y_true, y_pred)
                aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC = sentihood_AUC_Acc(y_true, score)
                max_per=aspect_strict_Acc     #ä»¥ä¸¥æ ¼çš„aspectå‡†ç¡®ç‡ä½œä¸ºæœ€ä½³æŒ‡æ ‡
            logger.info(" epoch : {0}, training loss: {1} ".format(str(epoch), loss_total/setp_total  ))  #æ‰“å°å½“å‰è½®æ¬¡çš„è®­ç»ƒæŸå¤±
                        
            # -------------------------- æ‰“å°éªŒè¯é›†æŒ‡æ ‡ ---------------------
            if self.opt.dataset == 'semeval':  
                logger.info('')
                logger.info('>> P: {:.4f} , R: {:.4f} , F: {:.4f} '.format(aspect_P, aspect_R, aspect_F))
                logger.info('>> 2 classes acc: {:.4f} '.format(sentiment_Acc_2_classes))
                logger.info('>> 3 classes acc: {:.4f} '.format(sentiment_Acc_3_classes))
                logger.info('>> 4 classes acc: {:.4f} '.format(sentiment_Acc_4_classes))
            else:  
                logger.info('')
                logger.info('>> aspect_strict_Acc: {:.4f} , aspect_Macro_F1: {:.4f} , aspect_Macro_AUC: {:.4f} '.format(
                    aspect_strict_Acc, 
                    aspect_Macro_F1, 
                    aspect_Macro_AUC
                ))
                logger.info('>> sentiment_Acc: {:.4f} '.format(sentiment_Acc))
                logger.info('>> sentiment_Macro_AUC: {:.4f} '.format(sentiment_Macro_AUC))

                
            # --------------------- ä¿å­˜æœ€ä¼˜æ¨¡å‹ ------------------------------
            if max_per > max_val_f1:  #å¦‚æœå½“å‰è¯„ä¼°æŒ‡æ ‡ä¼˜äºå†å²æœ€ä½³
                max_val_f1 = max_per   #æ›´æ–°æœ€ä½³æŒ‡æ ‡
                best_epoch = epoch    #è®°å½•æœ€ä¼˜è½®æ¬¡
                #åˆ›å»ºç›®å½•(æ ¹ç›®å½•ã€å­ç›®å½•)
                if not os.path.exists('state_dict'):  #å¦‚æœä¿å­˜ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼ˆæ ¹ç›®å½•ï¼‰
                    os.mkdir('state_dict')
                save_dir = f'state_dict/{self.opt.dataset}'    #åˆ›å»ºæ•°æ®é›†å¯¹åº”çš„å­ç›®å½•
                if not os.path.exists(save_dir):
                    os.makedirs(save_dir)

                #å®šä¹‰ä¿å­˜è·¯å¾„
                best_model_path = f'{save_dir}/seed{self.opt.seed}.bm'
                #ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶
                torch.save(self.model.state_dict(), best_model_path)
                #è®°å½•æœ€ä½³æ¨¡å‹çŠ¶æ€
                best_model_state = copy.deepcopy(self.model.state_dict())
               
            self.model.train()   #å°†æ¨¡å‹é‡æ–°è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        logger.info(f"ğŸ”¥ Training Finished. Best Epoch = {best_epoch}")
        return best_model_state,best_epoch   #è¿”å›æ¨¡å‹çŠ¶æ€å­—å…¸ã€æœ€ä¼˜è½®æ¬¡

        
    
    def _evaluate_acc_f1(self, data_loader):   
        """ æ ¸å¿ƒè¯„ä¼°å‡½æ•°ï¼šè·å–è¯„ä¼°æ•°æ®é›†çš„çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾ã€é¢„æµ‹åˆ†æ•°ï¼ˆç”¨äºåç»­è®¡ç®—å„ç±»æŒ‡æ ‡ï¼‰ """
        n_correct, n_total = 0, 0  #æ­£ç¡®é¢„æµ‹æ•°å’Œæ€»æ ·æœ¬æ•°
        t_targets_all, t_outputs_all = None, None   #å­˜å‚¨æ‰€æœ‰ç›®æ ‡æ ‡ç­¾å’Œè¾“å‡º
        score = []  #å­˜å‚¨é¢„æµ‹åˆ†æ•°
        self.model.eval()  #å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        with torch.no_grad():  #å…³é—­æ¢¯åº¦è®¡ç®—
            for t_batch, t_sample_batched in enumerate(data_loader):  #éå†æ•°æ®åŠ è½½å™¨
                t_sample_batched = [b.to(self.opt.device) for b in t_sample_batched]   #å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                input_ids, token_type_ids, attention_mask, labels = t_sample_batched   #è§£åŒ…æ‰¹æ¬¡æ•°æ®
                logits = self.model(input_ids, token_type_ids, attention_mask, labels=None)   #æ¨¡å‹å‘å‰ä¼ æ’­ï¼Œå¾—åˆ°logits
                score.append(F.softmax(logits, dim=-1).detach().cpu().numpy())  #å°†softmaxåçš„åˆ†æ•°æ·»åŠ åˆ°åˆ—è¡¨
                #è®¡ç®—æ­£ç¡®é¢„æµ‹æ•°å’Œæ€»æ ·æœ¬æ•°
                n_correct += (torch.argmax(logits, -1) == labels).sum().item()
                n_total += len(logits)
                #ç´¯ç§¯ç›®æ ‡æ ‡ç­¾å’Œè¾“å‡º
                if t_targets_all is None:
                    t_targets_all = labels
                    t_outputs_all = logits
                else:
                    t_targets_all = torch.cat((t_targets_all, labels), dim=0)
                    t_outputs_all = torch.cat((t_outputs_all, logits), dim=0)
        return t_targets_all.cpu().numpy(), torch.argmax(t_outputs_all, -1).cpu().numpy(), np.concatenate(score, axis=0)  


    
    def make_weights_for_balanced_classes(self, labels, nclasses, fixed=False):  
        """ ä¸ºä¸å¹³è¡¡æ•°æ®é›†ç”Ÿæˆæ ·æœ¬æƒé‡ï¼ˆç”¨äºWeightedRandomSamplerï¼Œè§£å†³ç±»åˆ«åˆ†å¸ƒä¸å‡é—®é¢˜ï¼‰"""
        if fixed:  #å¦‚æœä½¿ç”¨å›ºå®šæƒé‡
            weight = [0] * len(labels)   #åˆå§‹åŒ–æƒé‡åˆ—è¡¨
            if nclasses == 3:   #å¦‚æœæ˜¯3åˆ†ç±» 
                for idx, val in enumerate(labels):  #ä¸ºæ¯ä¸ªæ ·æœ¬è®¾ç½®æƒé‡
                    if val == 0:
                        weight[idx] = 0.2
                    elif val == 1:
                        weight[idx] = 0.4
                    elif val == 2:
                        weight[idx] = 0.4
                return weight  #è¿”å›æƒé‡åˆ—è¡¨
            else:  #å¦‚æœæ˜¯å…¶ä»–åˆ†ç±»æƒ…å†µ
                for idx, val in enumerate(labels):  #ä¸ºæ¯ä¸ªæ ·æœ¬è®¾ç½®æƒé‡
                    if val == 0:
                        weight[idx] = 0.2
                    else:
                        weight[idx] = 0.4
                return weight   #è¿”å›æƒé‡åˆ—è¡¨
        else:  #å¦‚æœæ ¹æ®ç±»åˆ«é¢‘ç‡ç”Ÿæˆæƒé‡
            count = [0] * nclasses  #åˆå§‹åŒ–ç±»åˆ«è®¡æ•°åˆ—è¡¨
            for item in labels:  #ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
                count[item] += 1
            weight_per_class = [0.] * nclasses  #åˆå§‹åŒ–æ¯ä¸ªç±»åˆ«çš„æƒé‡
            N = float(sum(count))   #æ€»æ ·æœ¬æ•°
            
            for i in range(nclasses):  #è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡ï¼ˆæ€»æ ·æœ¬æ•°/ç±»åˆ«æ ·æœ¬æ•°ï¼‰
                weight_per_class[i] = N / float(count[i])
            weight = [0] * len(labels)   #åˆå§‹åŒ–æ ·æœ¬æƒé‡åˆ—è¡¨
            for idx, val in enumerate(labels):  #ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…å¯¹åº”ç±»åˆ«çš„æƒé‡
                weight[idx] = weight_per_class[val]
            return weight  #è¿”å›æ ·æœ¬æƒé‡åˆ—è¡¨

            
    
    # =========================================================
    # âœ… RUN = Train best + Test best
    # =========================================================  
    def run(self):  
        #å°†è®­ç»ƒé›†çš„æ ‡ç­¾è½¬åŒ–ä¸ºå¼ é‡
        all_label_ids= torch.tensor([f['label'] for f in self.trainset], dtype=torch.long)   
        
        #å°†è®­ç»ƒé›†è½¬åŒ–ä¸ºå¼ é‡æ•°æ®é›†ï¼ŒåŒ…å«æ–‡æœ¬çš„BERTç´¢å¼•ã€æ®µè½IDã€è¾“å…¥æ©ç å’Œæ ‡ç­¾
        self.trainset = TensorDataset(torch.tensor([f['text_bert_indices'] for f in self.trainset], dtype=torch.long), 
                                      torch.tensor([f['bert_segments_ids'] for f in self.trainset], dtype=torch.long), 
                                      torch.tensor([f['input_mask'] for f in self.trainset], dtype=torch.long),
                                      all_label_ids)  
        
        if self.opt.dataset == "semeval": 
            sampler_weights = self.make_weights_for_balanced_classes(all_label_ids, 5)   #ä¸ºsemevalæ•°æ®é›†åˆ›å»ºå¹³è¡¡ç±»åˆ«çš„é‡‡æ ·æƒé‡ï¼ˆ5ä¸ªç±»åˆ«ï¼‰
        else:  
            sampler_weights = self.make_weights_for_balanced_classes(all_label_ids, 3)   #ä¸ºsentihoodæ•°æ®é›†åˆ›å»ºå¹³è¡¡ç±»åˆ«çš„é‡‡æ ·æƒé‡ï¼ˆ3ä¸ªç±»åˆ«ï¼‰
            
        #åˆ›å»ºåŠ æƒéšæœºé‡‡æ ·å™¨ï¼Œç”¨äºå¹³è¡¡è®­ç»ƒé›†ä¸­çš„ç±»åˆ«
        train_sampler = WeightedRandomSampler(sampler_weights, len(self.trainset), replacement=True)   
        #åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼Œä½¿ç”¨æŒ‡å®šçš„è®­ç»ƒé›†ã€æ‰¹æ¬¡å¤§å°å’Œé‡‡æ ·å™¨
        train_data_loader= DataLoader(dataset=self.trainset, batch_size=self.opt.train_batch_size,sampler=train_sampler)

        
        #å°†æµ‹è¯•é›†è½¬æ¢ä¸ºå¼ é‡æ•°æ®é›†
        self.testset = TensorDataset(torch.tensor([f['text_bert_indices'] for f in self.testset], dtype=torch.long),
                                      torch.tensor([f['bert_segments_ids'] for f in self.testset], dtype=torch.long),
                                      torch.tensor([f['input_mask'] for f in self.testset], dtype=torch.long),
                                      torch.tensor([f['label'] for f in self.testset], dtype=torch.long))
        
        #å°†éªŒè¯é›†è½¬æ¢ä¸ºå¼ é‡æ•°æ®é›†
        self.valset = TensorDataset(torch.tensor([f['text_bert_indices'] for f in self.valset], dtype=torch.long),
                                     torch.tensor([f['bert_segments_ids'] for f in self.valset], dtype=torch.long),
                                     torch.tensor([f['input_mask'] for f in self.valset], dtype=torch.long),
                                     torch.tensor([f['label'] for f in self.valset], dtype=torch.long))
        #åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼Œä½¿ç”¨æµ‹è¯•é›†å’Œè¯„ä¼°æ‰¹æ¬¡å¤§å°ï¼Œä¸æ‰“ä¹±æ•°æ®
        test_data_loader = DataLoader(dataset=self.testset, batch_size=self.opt.eval_batch_size, shuffle=False)  
        #åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
        val_data_loader = DataLoader(dataset=self.valset, batch_size=self.opt.eval_batch_size, shuffle=False)


        #è®¡ç®—æ€»çš„è®­ç»ƒæ­¥é•¿ï¼šè®­ç»ƒæ•°æ®åŠ è½½å™¨çš„é•¿åº¦*è®­ç»ƒè½®æ¬¡
        num_train_steps = int(len(train_data_loader) * self.opt.num_epoch)   
        t_total = num_train_steps   # å°†æ€»çš„è®­ç»ƒæ­¥æ•°èµ‹å€¼ç»™t_total
        
        #åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼šä½¿ç”¨æŒ‡å®šçš„ä¼˜åŒ–å™¨ã€æ¨¡å‹å‚æ•°ã€å­¦ä¹ ç‡å’ŒL2æ­£åˆ™åŒ–
        optimizer= self.opt.optimizer(
            self.model.parameters(), 
            lr=self.opt.learning_rate,   
            weight_decay=self.opt.l2reg
        )  
        
        criterion = nn.CrossEntropyLoss()  #å®šä¹‰äº¤å‰ç†µæŸå¤±å‡½æ•°

        #è®­ç»ƒæ¨¡å‹å¹¶è·å–æœ€ä½³æ¨¡å‹çš„å‚æ•°
        best_model_state,best_epoch = self._train(optimizer,criterion, train_data_loader, val_data_loader, t_total)    
        #åˆ¤æ–­æœ€ä½³æ¨¡å‹çŠ¶æ€æ˜¯å¦ä¸ºç©ºï¼Œé¿å…åŠ è½½NoneæŠ¥é”™
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)  #åŠ è½½æœ€ä½³æ¨¡å‹çš„å‚æ•°
        else:
            logger.warning("æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯é›†æŒ‡æ ‡æœªè¶…è¿‡åˆå§‹å€¼ï¼‰ï¼Œå°†ä½¿ç”¨è®­ç»ƒæœ€åä¸€è½®çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•")

        
        self.model.eval()   #å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        # -------------------------- æµ‹è¯•é›†ä¸Šè¯„ä¼°å¹¶è¾“å‡ºæŒ‡æ ‡ ---------------------
        logger.info(f"ğŸ”¥ Testing Best Epoch = {best_epoch}")
        y_true, y_pred, score = self._evaluate_acc_f1(test_data_loader)  #åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œè·å–çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾å’Œåˆ†æ•°
        if self.opt.dataset=='semeval':  #å¦‚æœæ•°æ®é›†æ˜¯semevalï¼Œè®¡ç®—å¹¶è¾“å‡ºç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡
            aspect_P, aspect_R, aspect_F = semeval_PRF(y_true, y_pred)
            sentiment_Acc_4_classes = semeval_Acc(y_true, y_pred, score, 4)
            sentiment_Acc_3_classes = semeval_Acc(y_true, y_pred, score, 3)
            sentiment_Acc_2_classes = semeval_Acc(y_true, y_pred, score, 2)
            logger.info('>> P: {:.4f} , R: {:.4f} , F: {:.4f} '.format(aspect_P, aspect_R, aspect_F))
            logger.info('>> 4 classes acc: {:.4f} '.format(sentiment_Acc_4_classes))
            logger.info('>> 3 classes acc: {:.4f} '.format(sentiment_Acc_3_classes))
            logger.info('>> 2 classes acc: {:.4f} '.format(sentiment_Acc_2_classes))
        else:   #å¦‚æœæ•°æ®é›†æ˜¯sentihoodï¼Œè®¡ç®—å¹¶è¾“å‡ºç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡
            aspect_strict_Acc = sentihood_strict_acc(y_true, y_pred)
            aspect_Macro_F1 = sentihood_macro_F1(y_true, y_pred)
            aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC = sentihood_AUC_Acc(y_true, score)
            logger.info(())
            logger.info('>> aspect_strict_Acc: {:.4f} , aspect_Macro_F1: {:.4f} , aspect_Macro_AUC: {:.4f} '.format(
                aspect_strict_Acc, 
                aspect_Macro_F1, 
                aspect_Macro_AUC
            ))
            logger.info('>> sentiment_Acc: {:.4f} '.format(sentiment_Acc))
            logger.info('>> sentiment_Macro_AUC: {:.4f} '.format(sentiment_Macro_AUC))
    


def main():
    # Hyper Parametersï¼ˆè¶…å‚æ•°è®¾ç½®ï¼‰
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', default='semeval',  choices=['semeval','sentihood'], type=str, required=True)   #æ•°æ®é›†
    parser.add_argument('--learning-rate', default=3e-5, type=float, help='try 5e-5, 2e-5')  #å­¦ä¹ ç‡
    parser.add_argument('--dropout', default=0.1, type=float)  #dropoutç‡
    parser.add_argument('--l2reg', default=0.001, type=float)  #L2æ­£åˆ™åŒ–ç³»æ•°
    parser.add_argument('--warmup-proportion', default=0.01, type=float)   #å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹
    parser.add_argument('--num_epoch', default=5, type=int, help='')  #è®­ç»ƒè½®æ•°
    parser.add_argument("--train-batch-size", default=32,type=int, help="Total batch size for training.")  #è®­ç»ƒæ‰¹æ¬¡å¤§å°
    parser.add_argument("--eval-batch-size", default=64, type=int, help="Total batch size for eval.")      #è¯„ä¼°æ‰¹æ¬¡å¤§å°
    parser.add_argument('--log-step', default=50, type=int)    #æ—¥å¿—è¾“å‡ºé—´éš”
    #é¢„è®­ç»ƒBERTæ¨¡å‹åç§°ï¼šé»˜è®¤bert-base-uncasedï¼ˆåŸºç¡€ç‰ˆæ— å¤§å°å†™åŒºåˆ†æ¨¡å‹ï¼‰
    parser.add_argument('--pretrained_bert_name', default='bert-base-uncased', type=str)  
    
    parser.add_argument('--max_seq_len', default=120, type=int)  #æ–‡æœ¬çš„æœ€å¤§åºåˆ—é•¿åº¦
    parser.add_argument('--label-dim', default=5, type=int)   #æ ‡ç­¾ç»´åº¦
    parser.add_argument('--hops', default=3, type=int)   #è·³è½¬æ¬¡æ•°
    parser.add_argument('--save_model', default=0, type=int)   #æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹
    parser.add_argument('--device', default='cuda', type=str, help='e.g. cuda:0')   #è®­ç»ƒè®¾å¤‡
    parser.add_argument('--seed', default=42, type=int, help='set seed for reproducibility')  #éšæœºç§å­ï¼ˆè®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°ï¼‰
    parser.add_argument('--valset_ratio', default=0, type=float,   
                        help='set ratio between 0 and 1 for validation support')   #éªŒè¯é›†æ¯”ä¾‹ï¼ˆ0åˆ°1ä¹‹é—´ï¼‰
    opt = parser.parse_args()


    #å¦‚æœæ•°æ®é›†æ˜¯sentihoodï¼Œå°†æ ‡ç­¾ç»´åº¦è®¾ç½®ä¸º3
    if opt.dataset=='sentihood':
        opt.label_dim =3

        
    #å¦‚æœè®¾ç½®äº†éšæœºç§å­ï¼Œä¸ºå„ç§éšæœºæ•°ç”Ÿæˆå™¨è®¾ç½®ç§å­
    if opt.seed is not None:
        random.seed(opt.seed)
        numpy.random.seed(opt.seed)
        torch.manual_seed(opt.seed)
        torch.cuda.manual_seed(opt.seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    #å®šä¹‰æ•°æ®é›†æ–‡ä»¶è·¯å¾„
    dataset_files = {
        'train': '../../datasets/{}/bert_train.json'.format(opt.dataset),
        'test': '../../datasets/{}/bert_test.json'.format(opt.dataset),
        'val': '../../datasets/{}/bert_dev.json'.format(opt.dataset)
    }

    
    logger.info(opt.pretrained_bert_name)    #æ‰“å°é¢„è®­ç»ƒBERTæ¨¡å‹çš„åç§°
    opt.optimizer = AdamW    #è®¾ç½®ä¼˜åŒ–å™¨ä¸ºAdamW
    opt.dataset_file = dataset_files    #è®¾ç½®æ•°æ®é›†æ–‡ä»¶è·¯å¾„ 
    opt.inputs_cols = ['text_bert_indices', 'bert_segments_ids', 'input_mask', 'label']    #è®¾ç½®è¾“å…¥åˆ—åï¼ˆBERTç´¢å¼•ã€æ®µè½IDã€è¾“å…¥æ©ç ã€æ ‡ç­¾ï¼‰
    opt.initializer = torch.nn.init.xavier_uniform_    # è®¾ç½®åˆå§‹åŒ–æ–¹æ³•ä¸ºxavierå‡åŒ€åˆå§‹åŒ–
    opt.device = torch.device(opt.device if torch.cuda.is_available() else 'cpu')   #è®¾ç½®è®¾å¤‡ä¸ºæŒ‡å®šçš„cudaæˆ–cpu

    ins = Instructor(opt)   
    ins.run()


if __name__ == '__main__':
    main()
