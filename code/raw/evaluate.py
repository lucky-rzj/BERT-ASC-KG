# ===========================================================================================
# æ¨¡å‹è¯„ä¼°å…¥å£----åŠ è½½æ¨¡å‹ + æ•°æ® + è°ƒç”¨æŒ‡æ ‡
# ===========================================================================================
# åŠŸèƒ½ï¼šç”¨äºåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œé¢„æµ‹ï¼Œç„¶åè°ƒç”¨ evaluation.py ä¸­çš„æŒ‡æ ‡å‡½æ•°è¾“å‡ºæœ€ç»ˆæ€§èƒ½
# ===========================================================================================

import json  #ç”¨äºJSONæ•°æ®å¤„ç†
import logging  #ç”¨äºæ—¥å¿—è®°å½•
import argparse  #ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
import os  #ç”¨äºæ–‡ä»¶è·¯å¾„æ“ä½œ
import sys  #ç”¨äºç³»ç»Ÿç›¸å…³æ“ä½œ
import random  #ç”¨äºéšæœºæ•°ç”Ÿæˆ
import numpy   #å¯¼å…¥numpyåº“ï¼Œç”¨äºæ•°å€¼è®¡ç®—
import pandas as pd   #å¯¼å…¥pandasåº“ï¼Œå¹¶ç®€å†™ä¸ºpdï¼Œç”¨äºæ•°æ®å¤„ç†
from torch.utils.data.sampler import  WeightedRandomSampler   #ç”¨äºåŠ æƒéšæœºé‡‡æ ·å™¨
import torch  #å¯¼å…¥pytorchåº“ï¼Œç”¨äºæ·±åº¦å­¦ä¹ ç›¸å…³æ“ä½œ
from torch.utils.data import DataLoader, random_split, TensorDataset  #ç”¨äºæ•°æ®åŠ è½½å’Œå¤„ç†
import torch.nn.functional as F   #pytorchçš„å‡½æ•°å¼æ¥å£
import  numpy as np 
import copy  #ç”¨äºå¯¹è±¡å¤åˆ¶
from  tqdm import tqdm   #ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡
from transformers import AdamW   #ä»transformersåº“å¯¼å…¥AdamWä¼˜åŒ–å™¨
from transformers import AutoTokenizer   #ç”¨äºåŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†è¯å™¨

from evaluation import *  #å¯¼å…¥è¯„ä¼°æŒ‡æ ‡å‡½æ•°
from data_utils import  ABSADataset_absa_bert_semeval_json, ABSADataset_absa_bert_sentihood_json   
from MyModel import BERT_ASC_vanila   


logger = logging.getLogger()   #åˆ›å»ºæ—¥å¿—è®°å½•å™¨
logger.setLevel(logging.INFO)  #è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºINFO
logger.addHandler(logging.StreamHandler(sys.stdout))   #å°†æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å°



class Instructor:
    def __init__(self, opt): 
        self.opt = opt 
        #åˆå§‹åŒ–æ¨¡å‹å¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.model = BERT_ASC_vanila(opt)  
        self.model.to(self.opt.device)
        
        #åŠ è½½é¢„è®­ç»ƒåˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(opt.pretrained_bert_name)   

        #æ ¹æ®æ•°æ®é›†çš„ç±»å‹ï¼ŒåŠ è½½æµ‹è¯•é›†    
        if self.opt.dataset=='semeval':  
            self.testset = ABSADataset_absa_bert_semeval_json(opt.dataset_file['test'], tokenizer, opt)
        else:
            self.testset = ABSADataset_absa_bert_sentihood_json(opt.dataset_file['test'], tokenizer, opt)
        logger.info(' test {}'.format( len(self.testset)))    #æ‰“å°æµ‹è¯•é›†çš„å¤§å°

        #è‹¥ä½¿ç”¨GPUï¼Œæ‰“å°æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if opt.device.type == 'cuda':   
            logger.info('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=opt.device.index)))
        if torch.cuda.device_count() > 1:   #å¦‚æœæœ‰å¤šä¸ªGPUï¼Œä½¿ç”¨æ•°æ®å¹¶è¡Œ
            logger.info(f'Using {torch.cuda.device_count()} GPUs.')
            self.model = nn.DataParallel(self.model)
        self._print_args()   #æ‰“å°å‚æ•°ä¿¡æ¯

        
    #è®¡ç®—å¯è®­ç»ƒå‚æ•°å’Œä¸å¯è®­ç»ƒå‚æ•°
    def _print_args(self):   
        n_trainable_params, n_nontrainable_params = 0, 0
        for p in self.model.parameters():   #éå†æ¨¡å‹å‚æ•°
            n_params = torch.prod(torch.tensor(p.shape))   #è®¡ç®—å‚æ•°æ€»æ•°
            if p.requires_grad:
                n_trainable_params += n_params
            else:
                n_nontrainable_params += n_params
        logger.info(
            'n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))
        logger.info('> training arguments:')
        for arg in vars(self.opt):
            logger.info('>>> {0}: {1}'.format(arg, getattr(self.opt, arg)))


    #è¯„ä¼°å‡†ç¡®ç‡å’ŒF1åˆ†æ•°çš„æ–¹æ³•
    def _evaluate_acc_f1(self, data_loader):  
        n_correct, n_total = 0, 0   #æ­£ç¡®é¢„æµ‹æ•°å’Œæ€»æ ·æœ¬æ•°
        t_targets_all, t_outputs_all = None, None   #å­˜å‚¨æ‰€æœ‰çœŸå®æ ‡ç­¾å’Œè¾“å‡º
        score = []  #å­˜å‚¨é¢„æµ‹åˆ†æ•°
        self.model.eval()   #æ¨¡å‹è®¾ä¸ºè¯„ä¼°æ¨¡å‹
        with torch.no_grad():   #å…³é—­æ¢¯åº¦è®¡ç®—
            for t_batch, t_sample_batched in enumerate(tqdm(data_loader)):  #éå†æ•°æ®åŠ è½½å™¨
                #å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                t_sample_batched = [b.to(self.opt.device) for b in t_sample_batched]
                input_ids, token_type_ids, attention_mask, labels = t_sample_batched

                logits = self.model(input_ids, token_type_ids, attention_mask, labels=None)   #æ¨¡å‹å‘å‰ä¼ æ’­ï¼Œè·å–è¾“å‡º
                score.append(F.softmax(logits, dim=-1).detach().cpu().numpy())   #å­˜å‚¨softmaxåçš„æ¦‚ç‡åˆ†æ•°
                #è®¡ç®—æ­£ç¡®é¢„æµ‹æ•°
                n_correct += (torch.argmax(logits, -1) == labels).sum().item()
                n_total += len(logits)

                if t_targets_all is None:   #ç´¯ç§¯æ‰€æœ‰æ ‡ç­¾å’Œè¾“å‡º
                    t_targets_all = labels
                    t_outputs_all = logits
                else:
                    t_targets_all = torch.cat((t_targets_all, labels), dim=0)
                    t_outputs_all = torch.cat((t_outputs_all, logits), dim=0)
        #è¿”å›çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾å’Œåˆ†æ•°
        return t_targets_all.cpu().numpy(), torch.argmax(t_outputs_all, -1).cpu().numpy(), np.concatenate(score, axis=0)


        
    # ---------------------- æµ‹è¯•é›†ä¸Šè¯„ä¼° -------------------------
    def run(self):  
        #è½¬æ¢æµ‹è¯•é›†ä¸ºTensorDatasetæ ¼å¼
        testset = TensorDataset(torch.tensor([f['text_bert_indices'] for f in self.testset], dtype=torch.long),
                                      torch.tensor([f['bert_segments_ids'] for f in self.testset], dtype=torch.long),
                                      torch.tensor([f['input_mask'] for f in self.testset], dtype=torch.long),
                                      torch.tensor([f['label'] for f in self.testset], dtype=torch.long)
        )
        #åˆ›å»ºæµ‹è¯•é›†æ•°æ®åŠ è½½å™¨
        test_data_loader = DataLoader(dataset=testset, batch_size=self.opt.eval_batch_size, shuffle=False)  

        
        # --------------------------------------------------
        # ğŸ”§ ç»Ÿä¸€ SEEDSï¼ˆåº”ä¸ä½ è®­ç»ƒç«¯ä½¿ç”¨çš„å®Œå…¨ä¸€è‡´ï¼‰
        # --------------------------------------------------
        if self.opt.dataset == 'semeval':
            SEEDS = [42, 21, 7, 13, 87]
        else:
            SEEDS = [42, 101, 735, 2025, 12345]
            
        all_results = []    #ä¿å­˜æ¯ä¸ª seed çš„ç»“æœ


        # --------------------------------------------------
        # â­ ä¾æ¬¡åŠ è½½æ¯ä¸ª seed çš„æ¨¡å‹ + åšæµ‹è¯•
        # --------------------------------------------------
        for seed in SEEDS:
            best_model_path = f"state_dict/{self.opt.dataset}/seed{seed}.bm"
            logger.info(f"\n============ Evaluating seed {seed} ============\n")
            #åŠ è½½æ¨¡å‹å‚æ•°
            self.model.load_state_dict(torch.load(best_model_path, map_location=self.opt.device))

            #------------------ æµ‹è¯•é›†ä¸Šè¯„ä¼°å¹¶æ‰“å°æŒ‡æ ‡ --------------------------------
            self.model.eval()  
            y_true, y_pred, score = self._evaluate_acc_f1(test_data_loader)  #è¯„ä¼°æ¨¡å‹ï¼Œè·å–çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾å’Œåˆ†æ•°
            if self.opt.dataset=='semeval':
                aspect_P, aspect_R, aspect_F = semeval_PRF(y_true, y_pred)
                sentiment_Acc_4_classes = semeval_Acc(y_true, y_pred, score, 4)
                sentiment_Acc_3_classes = semeval_Acc(y_true, y_pred, score, 3)
                sentiment_Acc_2_classes = semeval_Acc(y_true, y_pred, score, 2)

                logger.info("*************************************************************************")
                logger.info('>> P: {:.4f} , R: {:.4f} , F: {:.4f} '.format(aspect_P, aspect_R, aspect_F))
                logger.info('>> 4 classes acc: {:.4f} '.format(sentiment_Acc_4_classes))
                logger.info('>> 3 classes acc: {:.4f} '.format(sentiment_Acc_3_classes))
                logger.info('>> 2 classes acc: {:.4f} '.format(sentiment_Acc_2_classes))
                logger.info("*************************************************************************")

                #è®°å½•
                all_results.append({
                    "P": aspect_P,
                    "R": aspect_R,
                    "F": aspect_F,
                    "Acc4": sentiment_Acc_4_classes,
                    "Acc3": sentiment_Acc_3_classes,
                    "Acc2": sentiment_Acc_2_classes,
                })
            else:
                aspect_strict_Acc = sentihood_strict_acc(y_true, y_pred)
                aspect_Macro_F1 = sentihood_macro_F1(y_true, y_pred)
                aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC = sentihood_AUC_Acc(y_true, score)

                logger.info("*************************************************************************")
                logger.info(())
                logger.info('>> aspect_strict_Acc: {:.4f} , aspect_Macro_F1: {:.4f} , aspect_Macro_AUC: {:.4f} '.format(
                    aspect_strict_Acc, 
                    aspect_Macro_F1, 
                    aspect_Macro_AUC
                ))
                logger.info('>> sentiment_Acc: {:.4f} '.format(sentiment_Acc))
                logger.info('>> sentiment_Macro_AUC: {:.4f} '.format(sentiment_Macro_AUC))
                logger.info("*************************************************************************")

                all_results.append({
                    "aspect_strict_Acc": aspect_strict_Acc,
                    "aspect_Macro_F1": aspect_Macro_F1,
                    "aspect_Macro_AUC": aspect_Macro_AUC,
                    "sentiment_Acc": sentiment_Acc,
                    "sentiment_Macro_AUC": sentiment_Macro_AUC,
                })

                
        # --------------------------------------------------
        # â­â­ è¾“å‡º 5 æ¬¡çš„å¹³å‡ç»“æœï¼ˆå¤ç°è®ºæ–‡ï¼‰
        # --------------------------------------------------
        logger.info("\n==================== 5-SEED AVERAGE ====================")
        avg = {k: sum(r[k] for r in all_results) / len(all_results) for k in all_results[0]}
        if self.opt.dataset == 'semeval':
            logger.info(f"Avg P     = {avg['P']:.4f}")
            logger.info(f"Avg R     = {avg['R']:.4f}")
            logger.info(f"Avg F     = {avg['F']:.4f}")
            logger.info(f"Avg Acc-4 = {avg['Acc4']:.4f}")
            logger.info(f"Avg Acc-3 = {avg['Acc3']:.4f}")
            logger.info(f"Avg Acc-2 = {avg['Acc2']:.4f}")
        else:
            logger.info(f"Avg strict_acc = {avg['aspect_strict_Acc']:.4f}")
            logger.info(f"Avg macro_F1   = {avg['aspect_Macro_F1']:.4f}")
            logger.info(f"Avg macro_AUC  = {avg['aspect_Macro_AUC']:.4f}")
            logger.info(f"Avg sent_Acc   = {avg['sentiment_Acc']:.4f}")
            logger.info(f"Avg sent_AUC   = {avg['sentiment_Macro_AUC']:.4f}")
        logger.info("=======================================================================")




def main():
    # Hyper Parameters
    parser = argparse.ArgumentParser()  #è§£æå‘½ä»¤è¡Œå‚æ•°
    parser.add_argument('--dataset', default='semeval', type=str,  choices=['semeval','sentihood'], help='semeval, sentihood', required=True)  
    parser.add_argument('--initializer', default='xavier_uniform_', type=str)    #
    parser.add_argument('--learning_rate', default=3e-5, type=float, help='try 5e-5, 2e-5')  #å­¦ä¹ ç‡
    parser.add_argument('--dropout', default=0.1, type=float)  
    parser.add_argument('--l2reg', default=0.001, type=float)  #L2æ­£åˆ™åŒ–
    parser.add_argument('--warmup_proportion', default=0.01, type=float)  #å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹
    parser.add_argument('--num_epoch', default=5, type=int, help='')  #è®­ç»ƒè½®æ•°
    parser.add_argument("--train_batch_size", default=32,type=int, help="Total batch size for training.")  #è®­ç»ƒçš„æ€»æ‰¹æ¬¡å¤§å°
    parser.add_argument("--eval_batch_size", default=64, type=int, help="Total batch size for eval.")  #è¯„ä¼°çš„æ€»æ‰¹æ¬¡å¤§å°
    parser.add_argument('--log_step', default=50, type=int)
    parser.add_argument('--pretrained_bert_name', default='bert-base-uncased', type=str)
    parser.add_argument('--max_seq_len', default=120, type=int)  #æ–‡æœ¬çš„æœ€å¤§åºåˆ—é•¿åº¦
    parser.add_argument('--label_dim', default=5, type=int)  #æ ‡ç­¾ç»´åº¦
    parser.add_argument('--hops', default=3, type=int)
    parser.add_argument('--save_model', default=0, type=int)   #ä¿å­˜æœ€ä½³æ¨¡å‹çš„è®¾ç½®
    parser.add_argument('--device', default='cuda', type=str, help='e.g. cuda:0')
    parser.add_argument('--seed', default=42, type=int, help='set seed for reproducibility')  #éšæœºç§å­ï¼ˆè®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°ï¼‰
    parser.add_argument('--valset_ratio', default=0, type=float,
                        help='set ratio between 0 and 1 for validation support')    #éªŒè¯æ¯”ä¾‹ï¼ˆè®¾ç½®0åˆ°1ä¹‹é—´çš„æ¯”ä¾‹ä½œä¸ºéªŒè¯é›†æ”¯æŒï¼‰
    opt = parser.parse_args()

    
    if opt.dataset=='sentihood':  #æ ¹æ®æ•°æ®é›†è®¾ç½®æ ‡ç­¾ç»´åº¦ï¼ˆSentihoodä¸º3ç±»ï¼ŒSemEvalä¸º5ç±»ï¼‰
        opt.label_dim =3
        
    #è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§
    if opt.seed is not None:
        random.seed(opt.seed)
        numpy.random.seed(opt.seed)
        torch.manual_seed(opt.seed)
        torch.cuda.manual_seed(opt.seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    #å®šä¹‰æ•°æ®é›†çš„æ–‡ä»¶è·¯å¾„
    dataset_files = {
        'train': '../../datasets/{}/bert_train.json'.format(opt.dataset),
        'test': '../../datasets/{}/bert_test.json'.format(opt.dataset),
        'val': '../../datasets/{}/bert_dev.json'.format(opt.dataset)
    }
    #å®šä¹‰å‚æ•°åˆå§‹åŒ–çš„æ–¹æ³•
    initializers = {
        'xavier_uniform_': torch.nn.init.xavier_uniform_,
        'xavier_normal_': torch.nn.init.xavier_normal,
        'orthogonal_': torch.nn.init.orthogonal_,
    }


    #è®¾ç½®é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„å’Œå…¶ä»–å‚æ•°
    logger.info(opt.pretrained_bert_name)  #æ‰“å°é¢„è®­ç»ƒBERTæ¨¡å‹çš„åç§°
    opt.dataset_file = dataset_files
    opt.inputs_cols = ['text_bert_indices', 'bert_segments_ids', 'input_mask', 'label']
    opt.initializer = initializers[opt.initializer]
    opt.device = torch.device(opt.device if torch.cuda.is_available() else 'cpu') 
    
    ins = Instructor(opt)   # # åˆ›å»ºInstructorå®ä¾‹å¹¶æ‰§è¡Œè¯„ä¼°
    ins.run()


if __name__ == '__main__':
    main()
