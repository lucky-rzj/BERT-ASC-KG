# =================================================================================
# è®­ç»ƒä¸»ç¨‹åº----æ‰§è¡Œè®­ç»ƒ â†’ éªŒè¯ â†’ æµ‹è¯• â†’ ä¿å­˜æ¨¡å‹
# =================================================================================
# åŠŸèƒ½ï¼šé¢å‘BERT-PTï¼ˆé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼‰+ å¤šå±‚è¡¨å¾èåˆæ¨¡å‹çš„è®­ç»ƒè„šæœ¬
# ================================================================================
import logging
import argparse
import os
import sys
import random
import numpy
import  numpy as np
from transformers import AdamW
import torch
from torch.utils.data.sampler import  WeightedRandomSampler   #å¯¼å…¥åŠ æƒéšæœºé‡‡æ ·å™¨ï¼ˆç”¨äºå¹³è¡¡æ•°æ®é›†ï¼‰
from torch.utils.data import DataLoader, random_split, TensorDataset
import torch.nn.functional as F
import copy
from  tqdm import tqdm


# --------------------- è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥ --------------------------
from data_utils_pt import(
    ABSATokenizer,      #è‡ªå®šä¹‰åˆ†è¯å™¨
    ABSADataset_absa_bert_semeval_json,    # SemEvalæ•°æ®é›†åŠ è½½å™¨ï¼ˆè§£æJSONæ ¼å¼çš„ABSAæ•°æ®ï¼‰
    ABSADataset_absa_bert_sentihood_json   # Sentihoodæ•°æ®é›†åŠ è½½å™¨
)  
from evaluation import *      #è¯„ä¼°å‡½æ•°
from MyModel_pt import BERT_ASC    #æ ¸å¿ƒæ¨¡å‹ï¼ˆBERT-PT + å¤šå±‚è¡¨å¾èåˆçš„ABSAåˆ†ç±»æ¨¡å‹ï¼‰


# ------------------------- æ—¥å¿—è®°å½• ------------------------------
logger = logging.getLogger()
logger.setLevel(logging.INFO)
logger.addHandler(logging.StreamHandler(sys.stdout))


# ------------------------- æ ¸å¿ƒè®­ç»ƒç±» ------------------------------
class Instructor:
    """è®­ç»ƒæŒ‡å¯¼ç±»ï¼šæ•´åˆABSAä»»åŠ¡çš„å…¨æµç¨‹ï¼ˆæ•°æ®åŠ è½½â†’æ¨¡å‹åˆå§‹åŒ–â†’è®­ç»ƒâ†’è¯„ä¼°â†’ä¿å­˜ï¼‰"""
    def __init__(self, opt):
        """
        åˆå§‹åŒ–å‡½æ•°ï¼šæ¥æ”¶é…ç½®å‚æ•°ï¼Œå®Œæˆæ•°æ®é›†åŠ è½½ã€æ¨¡å‹åˆå§‹åŒ–
        :param opt: å‘½ä»¤è¡Œè§£æåçš„é…ç½®å‚æ•°å¯¹è±¡ï¼ˆåŒ…å«æ‰€æœ‰è®­ç»ƒç›¸å…³è®¾ç½®ï¼‰
        """
        self.opt = opt
        
        # 1. åˆå§‹åŒ–æ¨¡å‹ï¼ˆABSAæ ¸å¿ƒæ¨¡å‹ï¼šBERT-PTé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ¨¡å‹ï¼‰
        #self.model = BERT_ASC.from_pretrained(opt.pt_model, num_labels=opt.label_dim)
        self.model = BERT_ASC.from_pretrained(opt.pretrained_bert_name, num_labels=opt.label_dim)
        self.model.to(self.opt.device)
        
        # 2. åˆå§‹åŒ–åˆ†è¯å™¨ï¼ˆä¸é¢„è®­ç»ƒæ¨¡å‹åŒ¹é…ï¼Œä¿è¯tokenizationä¸€è‡´æ€§ï¼‰
        #tokenizer = ABSATokenizer.from_pretrained(opt.pt_model)  
        tokenizer = ABSATokenizer.from_pretrained(opt.pretrained_bert_name)
        
        # 3. åŠ è½½æ•°æ®é›†ï¼ˆæ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©å¯¹åº”çš„åŠ è½½å™¨ï¼‰
        if self.opt.dataset=='semeval':
            self.trainset = ABSADataset_absa_bert_semeval_json(opt.dataset_file['train'], tokenizer, opt)
            self.testset = ABSADataset_absa_bert_semeval_json(opt.dataset_file['test'], tokenizer, opt)
            assert 0 <= opt.valset_ratio < 1
            if opt.valset_ratio > 0:
                valset_len = int(len(self.trainset) * opt.valset_ratio)
                self.trainset, self.valset = random_split(self.trainset, (len(self.trainset) - valset_len, valset_len))
            else:
                self.valset = self.testset
        else:
            self.trainset = ABSADataset_absa_bert_sentihood_json(opt.dataset_file['train'], tokenizer, opt)
            self.testset = ABSADataset_absa_bert_sentihood_json(opt.dataset_file['test'], tokenizer, opt)
            self.valset =self.testset
        logger.info('train {0}: dev {1}: test {2}'.format(len(self.trainset), len(self.valset), len(self.testset)))
        
        # 4. GPUæ˜¾å­˜ç›‘æ§ï¼ˆè‹¥ä½¿ç”¨GPUï¼Œè¾“å‡ºåˆå§‹æ˜¾å­˜å ç”¨ï¼‰
        if opt.device.type == 'cuda':
            logger.info('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=opt.device.index)))
        self._print_args()


    def _print_args(self):
        """è¾…åŠ©å‡½æ•°ï¼šæ‰“å°æ¨¡å‹å¯è®­ç»ƒå‚æ•°æ•°é‡å’Œæ‰€æœ‰è®­ç»ƒé…ç½®å‚æ•°"""
        n_trainable_params, n_nontrainable_params = 0, 0   #å¯è®­ç»ƒå‚æ•°/ä¸å¯è®­ç»ƒå‚æ•°è®¡æ•°
        for p in self.model.parameters():    #éå†æ¨¡å‹æ‰€æœ‰å‚æ•°
            n_params = torch.prod(torch.tensor(p.shape))   #è®¡ç®—å•ä¸ªå‚æ•°å¼ é‡çš„å…ƒç´ ä¸ªæ•°ï¼ˆå‚æ•°æ•°é‡ï¼‰
            if p.requires_grad:    #è‹¥å‚æ•°éœ€è¦æ¢¯åº¦æ›´æ–°ï¼ˆå¯è®­ç»ƒï¼‰
                n_trainable_params += n_params
            else:    #è‹¥å‚æ•°å›ºå®šï¼ˆä¸å¯è®­ç»ƒï¼Œå¦‚å†»ç»“çš„BERTåº•å±‚ï¼‰
                n_nontrainable_params += n_params
        #è¾“å‡ºå‚æ•°æ•°é‡
        logger.info(
            'n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))
        logger.info('> training arguments:')    #è¾“å‡ºæ‰€æœ‰è®­ç»ƒé…ç½®å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰ï¼‰
        for arg in vars(self.opt):   #éå†optçš„æ‰€æœ‰å±æ€§
            logger.info('>>> {0}: {1}'.format(arg, getattr(self.opt, arg)))   #è¾“å‡ºå‚æ•°åå’Œå€¼


    def warmup_linear(self, x, warmup=0.002):
        """
        çº¿æ€§å­¦ä¹ ç‡é¢„çƒ­å‡½æ•°ï¼šè®­ç»ƒåˆæœŸé€æ­¥æå‡å­¦ä¹ ç‡ï¼Œé¿å…æ¢¯åº¦éœ‡è¡
        :param x: å½“å‰æ­¥æ•°/æ€»æ­¥æ•°ï¼ˆæ¯”ä¾‹å€¼ï¼ŒèŒƒå›´[0,1]ï¼‰
        :param warmup: é¢„çƒ­æ¯”ä¾‹ï¼ˆé»˜è®¤0.002ï¼Œå³å‰0.2%æ­¥æ•°ä¸ºé¢„çƒ­é˜¶æ®µï¼‰
        :return: å­¦ä¹ ç‡ç¼©æ”¾å› å­
        """
        if x < warmup:
            return x / warmup   #é¢„çƒ­é˜¶æ®µï¼šå­¦ä¹ ç‡éšæ­¥æ•°çº¿æ€§å¢é•¿
        else:
            return max((1.0 - x), 0.0)   #é¢„çƒ­åï¼šå­¦ä¹ ç‡éšæ­¥æ•°çº¿æ€§è¡°å‡


    
    # =====================================================================
    # âœ… TRAIN (with validation and best epoch selection)
    # =====================================================================
    def _train(self, optimizer, train_data_loader, val_data_loader, t_total):
        """
        æ ¸å¿ƒè®­ç»ƒå‡½æ•°ï¼šæ‰§è¡Œå¤šè½®è®­ç»ƒã€å­¦ä¹ ç‡è°ƒåº¦ã€éªŒè¯é›†è¯„ä¼°ã€æœ€ä½³æ¨¡å‹ä¿å­˜
        :param optimizer: ä¼˜åŒ–å™¨ï¼ˆæ­¤å¤„ä¸ºAdamWï¼‰
        :param train_data_loader: è®­ç»ƒé›†æ•°æ®åŠ è½½å™¨
        :param val_data_loader: éªŒè¯é›†æ•°æ®åŠ è½½å™¨
        :param t_total: æ€»è®­ç»ƒæ­¥æ•°ï¼ˆæ‰¹æ¬¡æ•°é‡ Ã— è®­ç»ƒè½®æ•°ï¼‰
        :return: æœ€ä½³æ¨¡å‹çš„å‚æ•°å­—å…¸ï¼ˆstate_dictï¼‰
        """
        max_val_f1 = 0   #è®°å½•éªŒè¯é›†æœ€ä½³æ€§èƒ½æŒ‡æ ‡ï¼ˆSemEvalç”¨4ç±»æƒ…æ„Ÿå‡†ç¡®ç‡ï¼ŒSentihoodç”¨ä¸¥æ ¼å‡†ç¡®ç‡ï¼‰
        global_step = 0
        path = None       #æœ€ä½³æ¨¡å‹å‚æ•°ä¿å­˜è·¯å¾„ï¼ˆå®é™…ä¿å­˜çš„æ˜¯state_dictï¼Œæ­¤å¤„ç”¨pathæŒ‡ä»£ï¼‰
        best_epoch = -1   #è®°å½•æœ€ä¼˜è½®æ¬¡
        
        for epoch in range(self.opt.num_epoch):
            loss_total= 0
            step_total= 0    #ç´¯è®¡è®­ç»ƒæ ·æœ¬æ•°
            logger.info('>' * 100)
            logger.info('epoch: {}'.format(epoch))
            
            self.model.train()
            # ---------------------- éå†è®­ç»ƒé›† -------------------------
            for i_batch, sample_batched in enumerate(tqdm(train_data_loader)):
                optimizer.zero_grad()   #æ¢¯åº¦æ¸…é›¶ï¼ˆé¿å…å‰ä¸€è½®æ¢¯åº¦ç´¯ç§¯ï¼‰
                sample_batched= [b.to(self.opt.device) for b in sample_batched]
                # --------------------------- è§£åŒ…æ‰¹æ¬¡æ•°æ® -------------------------------------
                # input_idsï¼štokençš„ç´¢å¼•åºåˆ—ï¼ˆå¥å­+aspect-termçš„tokenåŒ–ç»“æœï¼‰
                # token_type_idsï¼šå¥å­åˆ†æ®µIDï¼ˆåŒºåˆ†å¥å­ä¸»ä½“å’Œaspect-termï¼Œ0/1æ ‡è¯†ï¼‰
                # attention_maskï¼šæ³¨æ„åŠ›æ©ç ï¼ˆå¿½ç•¥padding tokenï¼Œ1è¡¨ç¤ºæœ‰æ•ˆtokenï¼Œ0è¡¨ç¤ºpaddingï¼‰
                # labelsï¼šæƒ…æ„Ÿæ ‡ç­¾ï¼ˆSemEvalï¼š0-4ï¼ŒSentihoodï¼š0-2ï¼‰
                # -----------------------------------------------------------------------------
                input_ids, token_type_ids, attention_mask, labels= sample_batched

                
                #æ¨¡å‹å‰å‘ä¼ æ’­ï¼šè®¡ç®—æŸå¤±ï¼ˆè®­ç»ƒæ¨¡å¼ä¸‹ä¼ å…¥labelsï¼Œæ¨¡å‹å†…éƒ¨è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼‰
                loss= self.model(input_ids, token_type_ids, attention_mask, labels)
                loss.backward()    #åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
                #ç´¯è®¡æŸå¤±å’Œæ ·æœ¬æ•°
                with torch.no_grad():
                    loss_total+= loss.item()
                    step_total+=len(labels)

                #è®¡ç®—å½“å‰æ­¥æ•°çš„å­¦ä¹ ç‡ï¼ˆçº¿æ€§é¢„çƒ­+çº¿æ€§è¡°å‡ï¼‰
                lr_this_step = self.opt.learning_rate * self.warmup_linear(global_step / t_total, self.opt.warmup_proportion)
                for param_group in optimizer.param_groups:
                    param_group['lr'] = lr_this_step   #æ›´æ–°æ‰€æœ‰å‚æ•°ç»„çš„å­¦ä¹ ç‡
                optimizer.step()   #æ¢¯åº¦ä¸‹é™ï¼šæ›´æ–°æ¨¡å‹å‚æ•°
                global_step += 1

            
            logger.info(" epoch : {0}, training loss: {1} ".format(str(epoch), loss_total / step_total))   #è¾“å‡ºå½“å‰è½®æ¬¡çš„å¹³å‡è®­ç»ƒæŸå¤±
            #-------------------------- åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å¹¶æ‰“å°éªŒè¯é›†æŒ‡æ ‡ ---------------------
            y_true, y_pred, score = self._evaluate_acc_f1(val_data_loader)    #éªŒè¯é›†è¯„ä¼°ï¼šè·å–çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾ã€é¢„æµ‹åˆ†æ•°
            if self.opt.dataset  == 'semeval':
                aspect_P, aspect_R, aspect_F = semeval_PRF(y_true, y_pred)
                sentiment_Acc_4_classes = semeval_Acc(y_true, y_pred, score, 4)
                sentiment_Acc_3_classes = semeval_Acc(y_true, y_pred, score, 3)
                sentiment_Acc_2_classes = semeval_Acc(y_true, y_pred, score, 2)
                max_per = sentiment_Acc_4_classes    #ä»¥4ç±»æƒ…æ„Ÿå‡†ç¡®ç‡ä½œä¸ºæœ€ä½³æŒ‡æ ‡
                
                logger.info('')
                logger.info('>> P: {:.4f} , R: {:.4f} , F: {:.4f} '.format(aspect_P, aspect_R, aspect_F))
                logger.info('>> 2 classes acc: {:.4f} '.format(sentiment_Acc_2_classes))
                logger.info('>> 3 classes acc: {:.4f} '.format(sentiment_Acc_3_classes))
                logger.info('>> 4 classes acc: {:.4f} '.format(sentiment_Acc_4_classes))
            else:
                aspect_strict_Acc = sentihood_strict_acc(y_true, y_pred)
                aspect_Macro_F1 = sentihood_macro_F1(y_true, y_pred)
                aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC = sentihood_AUC_Acc(y_true, score)
                max_per = aspect_strict_Acc   #ä»¥ä¸¥æ ¼å‡†ç¡®ç‡ä½œä¸ºæœ€ä½³æŒ‡æ ‡

                logger.info('')
                logger.info('>> aspect_strict_Acc: {:.4f} , aspect_Macro_F1: {:.4f} , aspect_Macro_AUC: {:.4f} '.format(
                    aspect_strict_Acc,    
                    aspect_Macro_F1,
                    aspect_Macro_AUC
                ))
                logger.info('>> sentiment_Acc: {:.4f} '.format(sentiment_Acc))
                logger.info('>> sentiment_Macro_AUC: {:.4f} '.format(sentiment_Macro_AUC))


            # ------------------ Save best model ------------------            
            if max_per > max_val_f1:   #å½“å‰éªŒè¯é›†æ€§èƒ½ > å†å²æœ€ä½³
                max_val_f1 = max_per   #æ›´æ–°æœ€ä½³æŒ‡æ ‡
                best_epoch = epoch    #è®°å½•æœ€ä¼˜è½®æ¬¡
                if not os.path.exists('state_dict'):
                    os.mkdir('state_dict')
                path = copy.deepcopy(self.model.state_dict())   #æ·±æ‹·è´æ¨¡å‹å½“å‰å‚æ•°

            self.model.train()    #éªŒè¯ç»“æŸåï¼Œæ¨¡å‹æ¢å¤è®­ç»ƒæ¨¡å¼
        logger.info(f"ğŸ”¥ Training Finished. Best Epoch = {best_epoch}")
        return path, best_epoch    #è¿”å›æœ€ä½³æ¨¡å‹çš„å‚æ•°å­—å…¸ã€æœ€ä¼˜è½®æ¬¡


        
    # ------------------------------ æ ¸å¿ƒè¯„ä¼°å‡½æ•° ------------------------------
    def _evaluate_acc_f1(self, data_loader):
        """
        æ ¸å¿ƒè¯„ä¼°å‡½æ•°ï¼šè·å–è¯„ä¼°æ•°æ®é›†çš„çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾ã€é¢„æµ‹åˆ†æ•°ï¼ˆç”¨äºåç»­è®¡ç®—å„ç±»æŒ‡æ ‡ï¼‰
        :param data_loader: è¯„ä¼°æ•°æ®é›†åŠ è½½å™¨ï¼ˆéªŒè¯é›†/æµ‹è¯•é›†ï¼‰
        :return: çœŸå®æ ‡ç­¾æ•°ç»„ï¼ˆy_trueï¼‰ã€é¢„æµ‹æ ‡ç­¾æ•°ç»„ï¼ˆy_predï¼‰ã€é¢„æµ‹åˆ†æ•°æ•°ç»„ï¼ˆscoreï¼‰
        """
        n_correct, n_total = 0, 0    #æ­£ç¡®é¢„æµ‹æ•°ã€æ€»æ ·æœ¬æ•°
        t_targets_all, t_outputs_all = None, None    #ç´¯è®¡æ‰€æœ‰æ‰¹æ¬¡çš„çœŸå®æ ‡ç­¾ã€æ¨¡å‹è¾“å‡ºlogits
        score = []   #å­˜å‚¨æ¯ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹åˆ†æ•°
        self.model.eval()
        with torch.no_grad():
            #éå†è¯„ä¼°æ•°æ®é›†çš„æ‰€æœ‰æ‰¹æ¬¡
            for t_batch, t_sample_batched in enumerate(data_loader):
                t_sample_batched = [b.to(self.opt.device) for b in t_sample_batched]
                input_ids, token_type_ids, attention_mask, labels = t_sample_batched
                
                #æ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆé¢„æµ‹æ—¶ä¸ä¼ å…¥labelsï¼Œè¿”å›logitsï¼‰
                logits = self.model(input_ids, token_type_ids, attention_mask, labels=None)
                score.append(F.softmax(logits, dim=-1).detach().cpu().numpy())  #è®¡ç®—é¢„æµ‹åˆ†æ•°ï¼ˆlogitsç»è¿‡softmaxè½¬ä¸ºæ¦‚ç‡ï¼‰ï¼Œå¹¶æ·»åŠ åˆ°scoreåˆ—è¡¨

                n_correct += (torch.argmax(logits, -1) == labels).sum().item()
                n_total += len(logits)
                
                #æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡çš„çœŸå®æ ‡ç­¾å’Œlogits
                if t_targets_all is None:
                    t_targets_all = labels
                    t_outputs_all = logits
                else:
                    t_targets_all = torch.cat((t_targets_all, labels), dim=0)
                    t_outputs_all = torch.cat((t_outputs_all, logits), dim=0)
        #è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è¿”å›ï¼šçœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾ï¼ˆargmaxå–logitsæœ€å¤§å€¼ç´¢å¼•ï¼‰ã€é¢„æµ‹åˆ†æ•°ï¼ˆæ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡ï¼‰
        return t_targets_all.cpu().numpy(), torch.argmax(t_outputs_all, -1).cpu().numpy(), np.concatenate(score, axis=0)


    def make_weights_for_balanced_classes(self, labels, nclasses, fixed=False):
        """
        ä¸ºä¸å¹³è¡¡æ•°æ®é›†ç”Ÿæˆæ ·æœ¬æƒé‡ï¼ˆç”¨äºWeightedRandomSamplerï¼Œè§£å†³ç±»åˆ«åˆ†å¸ƒä¸å‡é—®é¢˜ï¼‰
        :param labels: æ ‡ç­¾åˆ—è¡¨ï¼ˆæˆ–å¼ é‡ï¼‰
        :param nclasses: ç±»åˆ«æ•°é‡
        :param fixed: æ˜¯å¦ä½¿ç”¨å›ºå®šæƒé‡ï¼ˆTrue=æ‰‹åŠ¨æŒ‡å®šæƒé‡ï¼ŒFalse=æŒ‰ç±»åˆ«é¢‘ç‡è‡ªåŠ¨è®¡ç®—æƒé‡ï¼‰
        :return: æ ·æœ¬æƒé‡åˆ—è¡¨
        """
        if fixed:   #æ‰‹åŠ¨å›ºå®šæƒé‡æ¨¡å¼
            weight = [0] * len(labels)
            if nclasses == 3:   #3åˆ†ç±»åœºæ™¯ï¼šæ ‡ç­¾0æƒé‡0.2ï¼Œæ ‡ç­¾1å’Œ2æƒé‡0.4ï¼ˆå¹³è¡¡å°‘æ•°ç±»ï¼‰
                for idx, val in enumerate(labels):
                    if val == 0:
                        weight[idx] = 0.2
                    elif val == 1:
                        weight[idx] = 0.4
                    elif val == 2:
                        weight[idx] = 0.4
                return weight
            else:   #å…¶ä»–åˆ†ç±»åœºæ™¯ï¼šæ ‡ç­¾0æƒé‡0.2ï¼Œå…¶ä»–æ ‡ç­¾æƒé‡0.4
                for idx, val in enumerate(labels):
                    if val == 0:
                        weight[idx] = 0.2
                    else:
                        weight[idx] = 0.4
                return weight
        else:   #è‡ªåŠ¨è®¡ç®—æƒé‡æ¨¡å¼
            count = [0] * nclasses
            for item in labels:
                idx = int(item)   #éå†æ ‡ç­¾ï¼Œè®¡æ•°æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
                count[idx] += 1
            weight_per_class = [0.] * nclasses   #æ¯ä¸ªç±»åˆ«çš„åŸºç¡€æƒé‡
            N = float(sum(count))   #è®­ç»ƒé›†æ€»æ ·æœ¬æ•°
            
            #è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡ï¼šæ€»æ ·æœ¬æ•° / è¯¥ç±»æ ·æœ¬æ•°ï¼ˆé¢‘ç‡è¶Šé«˜ï¼Œæƒé‡è¶Šä½ï¼‰
            for i in range(nclasses):
                weight_per_class[i] = N / float(count[i])
            weight = [0] * len(labels)  #åˆå§‹åŒ–æ ·æœ¬æƒé‡åˆ—è¡¨
            for idx, val in enumerate(labels):
                weight[idx] = weight_per_class[val]   #ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…å…¶æ‰€å±ç±»åˆ«çš„åŸºç¡€æƒé‡
            return weight


    
    # ====================================================================
    # âœ… RUN = Train best + Test best
    # =====================================================================
    def run(self):
        """
        è®­ç»ƒæµç¨‹ä¸»å‡½æ•°ï¼šæ•´åˆABSAä»»åŠ¡å…¨æµç¨‹ï¼ˆæ•°æ®é›†é¢„å¤„ç†â†’æ•°æ®åŠ è½½â†’ä¼˜åŒ–å™¨åˆå§‹åŒ–â†’è®­ç»ƒâ†’è¯„ä¼°â†’ä¿å­˜ï¼‰
        æ˜¯Instructorç±»çš„æ ¸å¿ƒå…¥å£ï¼Œè°ƒç”¨å…¶ä»–å‡½æ•°å®Œæˆç«¯åˆ°ç«¯è®­ç»ƒ
        """
        # 1. æå–è®­ç»ƒé›†æ‰€æœ‰æ ‡ç­¾å¹¶è½¬æ¢ä¸ºPyTorché•¿æ•´å‹å¼ é‡
        all_label_ids = torch.tensor([f['label'] for f in self.trainset], dtype=torch.long)   

        # 2. å°†è®­ç»ƒé›†è½¬æ¢ä¸ºTensorDatasetæ ¼å¼ï¼ˆPyTorchä¸“ç”¨æ•°æ®é›†æ ¼å¼ï¼Œä¾¿äºæ‰¹é‡åŠ è½½ï¼‰ï¼ŒåŒ…å«4ä¸ªå¼ é‡ï¼štokenç´¢å¼•ã€å¥å­åˆ†æ®µIDã€æ³¨æ„åŠ›æ©ç ã€æ ‡ç­¾å¼ é‡
        self.trainset = TensorDataset(
            torch.tensor([f['text_bert_indices'] for f in self.trainset], dtype=torch.long), 
            torch.tensor([f['bert_segments_ids'] for f in self.trainset], dtype=torch.long), 
            torch.tensor([f['input_mask'] for f in self.trainset], dtype=torch.long), 
            all_label_ids
        )
        
        # 3. ç”Ÿæˆè®­ç»ƒé›†æ ·æœ¬æƒé‡ï¼ˆè§£å†³ç±»åˆ«ä¸å¹³è¡¡ï¼‰
        if self.opt.dataset == "semeval":
            sampler_weights = self.make_weights_for_balanced_classes(all_label_ids, 5)
        else:
            sampler_weights = self.make_weights_for_balanced_classes(all_label_ids, 3)

        # 4. åˆå§‹åŒ–åŠ æƒéšæœºé‡‡æ ·å™¨ï¼ˆæŒ‰æ ·æœ¬æƒé‡é‡‡æ ·ï¼Œä½¿æ¯ä¸ªç±»åˆ«é‡‡æ ·æ¦‚ç‡å‡è¡¡ï¼‰
        train_sampler = WeightedRandomSampler(sampler_weights, len(self.trainset), replacement=True)
        # 5. æ„å»ºè®­ç»ƒé›†æ•°æ®åŠ è½½å™¨ï¼ˆæ‰¹é‡è¾“å‡ºè®­ç»ƒæ•°æ®ï¼Œå¸¦åŠ æƒé‡‡æ ·ï¼‰
        train_data_loader= DataLoader(dataset=self.trainset, batch_size=self.opt.train_batch_size,sampler=train_sampler)

        # 6. è½¬æ¢æµ‹è¯•é›†ä¸ºTensorDatasetæ ¼å¼
        self.testset = TensorDataset(torch.tensor([f['text_bert_indices'] for f in self.testset], dtype=torch.long),
                                      torch.tensor([f['bert_segments_ids'] for f in self.testset], dtype=torch.long),
                                      torch.tensor([f['input_mask'] for f in self.testset], dtype=torch.long),
                                      torch.tensor([f['label'] for f in self.testset], dtype=torch.long))
        # 7. è½¬æ¢éªŒè¯é›†ä¸ºTensorDatasetæ ¼å¼
        self.valset = TensorDataset(torch.tensor([f['text_bert_indices'] for f in self.valset], dtype=torch.long),
                                     torch.tensor([f['bert_segments_ids'] for f in self.valset], dtype=torch.long),
                                     torch.tensor([f['input_mask'] for f in self.valset], dtype=torch.long),
                                     torch.tensor([f['label'] for f in self.valset], dtype=torch.long))
        
        # 8. æ„å»ºæµ‹è¯•é›†/éªŒè¯é›†æ•°æ®åŠ è½½å™¨ï¼ˆè¯„ä¼°é˜¶æ®µæ— éœ€é‡‡æ ·ï¼Œshuffle=Falseé¿å…æ•°æ®æ··ä¹±ï¼‰
        test_data_loader = DataLoader(dataset=self.testset, batch_size=self.opt.eval_batch_size, shuffle=False)
        val_data_loader = DataLoader(dataset=self.valset, batch_size=self.opt.eval_batch_size, shuffle=False)

        # 9. è®¡ç®—æ€»è®­ç»ƒæ­¥æ•° = æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•°é‡ Ã— è®­ç»ƒè½®æ•°
        num_train_steps = int(len(train_data_loader) * self.opt.num_epoch)
        t_total = num_train_steps
        
        # 10. åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼šä¼ å…¥æ¨¡å‹å‚æ•°ã€å­¦ä¹ ç‡ã€L2æ­£åˆ™åŒ–ç³»æ•°
        optimizer= self.opt.optimizer(
            self.model.parameters(), 
            lr=self.opt.learning_rate,                        
            weight_decay=self.opt.l2reg
        )

        # 11. å¯åŠ¨è®­ç»ƒï¼šè°ƒç”¨_trainå‡½æ•°ï¼Œè¿”å›æœ€ä½³æ¨¡å‹çš„å‚æ•°å­—å…¸ï¼ˆstate_dictï¼‰
        best_model_path, best_epoch = self._train(optimizer, train_data_loader, val_data_loader, t_total)
        # 12. åŠ è½½æœ€ä½³æ¨¡å‹å‚æ•°ï¼ˆåŸºäºéªŒè¯é›†æ€§èƒ½çš„æœ€ä¼˜å‚æ•°ï¼Œç”¨äºæœ€ç»ˆæµ‹è¯•ï¼‰
        self.model.load_state_dict(best_model_path)


        # 13. æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ï¼ˆç”¨æœ€ä½³æ¨¡å‹è®¡ç®—æµ‹è¯•é›†æŒ‡æ ‡ï¼Œä½œä¸ºæœ€ç»ˆç»“æœï¼‰
        self.model.eval()
        #-------------------------- æµ‹è¯•é›†ä¸Šè¯„ä¼°å¹¶è¾“å‡ºæŒ‡æ ‡ ---------------------
        logger.info(f"ğŸ”¥ Testing Best Epoch = {best_epoch}")
        y_true, y_pred, score = self._evaluate_acc_f1(test_data_loader)     #åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œè·å–çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾å’Œåˆ†æ•°
        if self.opt.dataset=='semeval':
            aspect_P, aspect_R, aspect_F = semeval_PRF(y_true, y_pred)
            sentiment_Acc_4_classes = semeval_Acc(y_true, y_pred, score, 4)
            sentiment_Acc_3_classes = semeval_Acc(y_true, y_pred, score, 3)
            sentiment_Acc_2_classes = semeval_Acc(y_true, y_pred, score, 2)
            
            logger.info('>> P: {:.4f} , R: {:.4f} , F: {:.4f} '.format(aspect_P, aspect_R, aspect_F))
            logger.info('>> 4 classes acc: {:.4f} '.format(sentiment_Acc_4_classes))
            logger.info('>> 3 classes acc: {:.4f} '.format(sentiment_Acc_3_classes))
            logger.info('>> 2 classes acc: {:.4f} '.format(sentiment_Acc_2_classes))
        else:
            aspect_strict_Acc = sentihood_strict_acc(y_true, y_pred)
            aspect_Macro_F1 = sentihood_macro_F1(y_true, y_pred)
            aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC = sentihood_AUC_Acc(y_true, score)
            
            logger.info('>> aspect_strict_Acc: {:.4f} , aspect_Macro_F1: {:.4f} , aspect_Macro_AUC: {:.4f} '.format(
                    aspect_strict_Acc,    
                    aspect_Macro_F1,
                    aspect_Macro_AUC
            ))
            logger.info('>> sentiment_Acc: {:.4f} '.format(sentiment_Acc))
            logger.info('>> sentiment_Macro_AUC: {:.4f} '.format(sentiment_Macro_AUC))


            
        # ------------------------  æŒ‰ seed ä¿å­˜æ¨¡å‹ ----------------------------------
        # 14. ä¿å­˜æœ€ä½³æ¨¡å‹å‚æ•°ï¼ˆè‹¥å¯ç”¨save_model=1ï¼‰
        if self.opt.save_model:
            #æ„å»ºæ¨¡å‹ä¿å­˜è·¯å¾„ï¼šstate_dict/æ•°æ®é›†å/seed.bm
            save_path = f"/hy-tmp/BERT-ASC-main/code/PT/state_dict/{self.opt.dataset}/seed{self.opt.seed}.bm"
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            torch.save(self.model.state_dict(), save_path)   #ä¿å­˜æ¨¡å‹å‚æ•°å­—å…¸
            logger.info(f"ğŸ’¾ Model saved to {save_path}")


def main():
    """
    ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°ã€è®¾ç½®éšæœºç§å­ã€é…ç½®æ•°æ®é›†è·¯å¾„ã€åˆå§‹åŒ–è®­ç»ƒæŒ‡å¯¼ç±»å¹¶å¯åŠ¨è®­ç»ƒ
    """
    # Hyper Parameters
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', default='semeval',  choices=['semeval','sentihood'], type=str, required=True)
    parser.add_argument('--learning_rate', default=3e-5, type=float, help='try 5e-5, 2e-5')
    parser.add_argument('--dropout', default=0.1, type=float)
    parser.add_argument('--l2reg', default=0.001, type=float)
    parser.add_argument('--warmup_proportion', default=0.01, type=float)
    parser.add_argument('--num_epoch', default=5, type=int, help='')
    parser.add_argument("--train_batch_size", default=32,type=int, help="Total batch size for training.")
    parser.add_argument("--eval_batch_size", default=64, type=int, help="Total batch size for eval.")
    parser.add_argument('--log_step', default=50, type=int)
    parser.add_argument('--max_seq_len', default=120, type=int)
    parser.add_argument('--label_dim', default=5, type=int)
    parser.add_argument('--hops', default=3, type=int)
    #é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„/åç§°ï¼šé»˜è®¤activebus/BERT-PT_restï¼ˆé¤é¥®é¢†åŸŸé¢„è®­ç»ƒæ¨¡å‹ï¼‰
    #parser.add_argument('--pt_model', default='activebus/bert-pt_rest', type=str)
    parser.add_argument('--pretrained_bert_name', default='bert-base-uncased', type=str) 
    parser.add_argument('--save_model', default=0, type=int)
    parser.add_argument('--device', default='cuda', type=str, help='e.g. cuda:0')
    parser.add_argument('--seed', default=42, type=int, help='set seed for reproducibility')
    parser.add_argument('--valset_ratio', default=0, type=float,
                        help='set ratio between 0 and 1 for validation support')
    opt = parser.parse_args()


    if opt.dataset=='sentihood':
        opt.label_dim =3

        
    #è®¾ç½®éšæœºç§å­ï¼ˆä¿è¯ç»“æœå¯å¤ç°ï¼‰
    if opt.seed is not None:
        random.seed(opt.seed)
        numpy.random.seed(opt.seed)
        torch.manual_seed(opt.seed)
        torch.cuda.manual_seed(opt.seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

        
    #é…ç½®æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆè®­ç»ƒé›†ã€æµ‹è¯•é›†ã€éªŒè¯é›†ï¼‰
    dataset_files = {
        'train': '../../datasets/{}/bert_train.json'.format(opt.dataset),
        'test': '../../datasets/{}/bert_test.json'.format(opt.dataset),
        'val': '../../datasets/{}/bert_dev.json'.format(opt.dataset)
    }


    #logger.info(opt.pt_model)                 #PLM---bert-pt-rest
    logger.info(opt.pretrained_bert_name)      #PLM---bert-base-uncased/bert-large-uncased
    opt.optimizer = AdamW
    opt.model_class = ABSATokenizer
    opt.dataset_file = dataset_files
    opt.inputs_cols = ['text_bert_indices', 'bert_segments_ids', 'input_mask', 'label']   #é…ç½®æ¨¡å‹è¾“å…¥åˆ—åï¼ˆä¸æ•°æ®é›†å¼ é‡å¯¹åº”ï¼‰
    opt.initializer = torch.nn.init.xavier_uniform_
    opt.device = torch.device(opt.device if torch.cuda.is_available() else 'cpu')

    ins = Instructor(opt)
    ins.run()


if __name__ == '__main__':
    main()
