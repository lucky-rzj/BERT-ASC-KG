# ======================================================================
# è®­ç»ƒä¸»ç¨‹åº----æ‰§è¡Œè®­ç»ƒ â†’ éªŒè¯ â†’ æµ‹è¯• â†’ ä¿å­˜æ¨¡å‹
# ======================================================================
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
import logging
import argparse
import sys
import random
import numpy
import  numpy as np
from transformers import AdamW
import torch
from torch.utils.data.sampler import  WeightedRandomSampler    #å¯¼å…¥åŠ æƒéšæœºé‡‡æ ·å™¨
from torch.utils.data import DataLoader, random_split, TensorDataset
import torch.nn.functional as F
import copy
from  tqdm import tqdm


# --------------------- è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥ --------------------------
from data_utils_plm import(ABSATokenizer, ABSADataset_absa_bert_semeval_json, ABSADataset_absa_bert_sentihood_json)  
from evaluation import *      
from MyModel_plm import PLM_ASC    


# ------------------------- æ—¥å¿—è®°å½• ------------------------------
logger = logging.getLogger()
logger.setLevel(logging.INFO)
logger.addHandler(logging.StreamHandler(sys.stdout))


# ------------------------- æ ¸å¿ƒè®­ç»ƒç±» ------------------------------
class Instructor:
    """è®­ç»ƒæŒ‡å¯¼ç±»ï¼šæ•´åˆABSAä»»åŠ¡çš„å…¨æµç¨‹ï¼ˆæ•°æ®åŠ è½½â†’æ¨¡å‹åˆå§‹åŒ–â†’è®­ç»ƒâ†’è¯„ä¼°â†’ä¿å­˜ï¼‰"""
    def __init__(self, opt):
        """åˆå§‹åŒ–å‡½æ•°ï¼šæ¥æ”¶é…ç½®å‚æ•°ï¼Œå®Œæˆæ•°æ®é›†åŠ è½½ã€æ¨¡å‹åˆå§‹åŒ–"""
        self.opt = opt
        
        # 1. åˆå§‹åŒ–æ¨¡å‹
        self.model = PLM_ASC(opt).to(opt.device)
        
        # 2. åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer = ABSATokenizer.from_pretrained(opt.pretrained_model)
        
        # 3. åŠ è½½æ•°æ®é›†
        if self.opt.dataset=='semeval':
            self.trainset = ABSADataset_absa_bert_semeval_json(opt.dataset_file['train'], tokenizer, opt)
            self.testset = ABSADataset_absa_bert_semeval_json(opt.dataset_file['test'], tokenizer, opt)
            assert 0 <= opt.valset_ratio < 1
            if opt.valset_ratio > 0:
                valset_len = int(len(self.trainset) * opt.valset_ratio)
                self.trainset, self.valset = random_split(self.trainset, (len(self.trainset) - valset_len, valset_len))
            else:
                self.valset = self.testset
        else:
            self.trainset = ABSADataset_absa_bert_sentihood_json(opt.dataset_file['train'], tokenizer, opt)
            self.testset = ABSADataset_absa_bert_sentihood_json(opt.dataset_file['test'], tokenizer, opt)
            self.valset =self.testset
        logger.info('train {0}: dev {1}: test {2}'.format(len(self.trainset), len(self.valset), len(self.testset)))
        
        # 4. GPUæ˜¾å­˜ç›‘æ§
        if opt.device.type == 'cuda':
            logger.info('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=opt.device.index)))
        self._print_args()


    def _print_args(self):
        """è¾…åŠ©å‡½æ•°ï¼šæ‰“å°æ¨¡å‹å¯è®­ç»ƒå‚æ•°æ•°é‡å’Œæ‰€æœ‰è®­ç»ƒé…ç½®å‚æ•°"""
        n_trainable_params, n_nontrainable_params = 0, 0   
        for p in self.model.parameters():    #éå†æ¨¡å‹æ‰€æœ‰å‚æ•°
            n_params = torch.prod(torch.tensor(p.shape))   #è®¡ç®—å•ä¸ªå‚æ•°å¼ é‡çš„å…ƒç´ ä¸ªæ•°
            if p.requires_grad:    #è‹¥å‚æ•°éœ€è¦æ¢¯åº¦æ›´æ–°ï¼ˆå¯è®­ç»ƒï¼‰
                n_trainable_params += n_params
            else:    #è‹¥å‚æ•°å›ºå®šï¼ˆä¸å¯è®­ç»ƒï¼‰
                n_nontrainable_params += n_params
        #è¾“å‡ºå‚æ•°æ•°é‡
        logger.info(
            'n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))
        logger.info('> training arguments:')   
        for arg in vars(self.opt):  
            logger.info('>>> {0}: {1}'.format(arg, getattr(self.opt, arg)))  


    def warmup_linear(self, x, warmup=0.002):
        """çº¿æ€§å­¦ä¹ ç‡é¢„çƒ­å‡½æ•°ï¼šè®­ç»ƒåˆæœŸé€æ­¥æå‡å­¦ä¹ ç‡ï¼Œé¿å…æ¢¯åº¦éœ‡è¡"""
        if x < warmup:
            return x / warmup   #é¢„çƒ­é˜¶æ®µï¼šå­¦ä¹ ç‡éšæ­¥æ•°çº¿æ€§å¢é•¿
        else:
            return max((1.0 - x), 0.0)   #é¢„çƒ­åï¼šå­¦ä¹ ç‡éšæ­¥æ•°çº¿æ€§è¡°å‡


    
    # =====================================================================
    # âœ… TRAIN (with validation and best epoch selection)
    # =====================================================================
    def _train(self, optimizer, train_data_loader, val_data_loader, t_total):
        """æ ¸å¿ƒè®­ç»ƒå‡½æ•°ï¼šæ‰§è¡Œå¤šè½®è®­ç»ƒã€å­¦ä¹ ç‡è°ƒåº¦ã€éªŒè¯é›†è¯„ä¼°ã€æœ€ä½³æ¨¡å‹ä¿å­˜"""
        max_val_f1 = 0   #è®°å½•éªŒè¯é›†æœ€ä½³æ€§èƒ½æŒ‡æ ‡
        global_step = 0
        path = None       #æœ€ä½³æ¨¡å‹å‚æ•°ä¿å­˜è·¯å¾„
        best_epoch = -1   #è®°å½•æœ€ä¼˜è½®æ¬¡
        
        for epoch in range(self.opt.num_epoch):
            loss_total= 0
            step_total= 0   
            logger.info('>' * 100)
            logger.info('epoch: {}'.format(epoch))
            self.model.train()
            
            from torch.cuda.amp import autocast, GradScaler
            scaler = GradScaler()
            
            # ---------------------- éå†è®­ç»ƒé›† -------------------------
            for i_batch, sample_batched in enumerate(tqdm(train_data_loader)):
                optimizer.zero_grad()   #æ¢¯åº¦æ¸…é›¶
                sample_batched= [b.to(self.opt.device) for b in sample_batched]
                input_ids, attention_mask, labels= sample_batched

                # ----------- AMP è‡ªåŠ¨æ··åˆç²¾åº¦å¼€å§‹ -----------
                with autocast():
                    loss = self.model(input_ids, attention_mask, labels)   #æ¨¡å‹å‰å‘ä¼ æ’­ï¼šè®¡ç®—æŸå¤±
                    
                # ----------- AMP è‡ªåŠ¨æ··åˆç²¾åº¦ç»“æŸ -----------
                    
                #åå‘ä¼ æ’­ï¼ˆä½¿ç”¨æ··åˆç²¾åº¦ scalerï¼‰
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
      
                #ç´¯è®¡æŸå¤±å’Œæ ·æœ¬æ•°
                with torch.no_grad():
                    loss_total+= loss.item()
                    step_total+=len(labels)

                    
                #è®¡ç®—å½“å‰æ­¥æ•°çš„å­¦ä¹ ç‡
                lr_this_step = self.opt.learning_rate * self.warmup_linear(global_step / t_total, self.opt.warmup_proportion)
                for param_group in optimizer.param_groups:
                    param_group['lr'] = lr_this_step   #æ›´æ–°æ‰€æœ‰å‚æ•°ç»„çš„å­¦ä¹ ç‡
                optimizer.step()   #æ¢¯åº¦ä¸‹é™ï¼šæ›´æ–°æ¨¡å‹å‚æ•°
                global_step += 1

            
            logger.info(" epoch : {0}, training loss: {1} ".format(str(epoch), loss_total / step_total))  
            #-------------------------- åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å¹¶æ‰“å°éªŒè¯é›†æŒ‡æ ‡ ---------------------
            y_true, y_pred, score = self._evaluate_acc_f1(val_data_loader)    #éªŒè¯é›†è¯„ä¼°ï¼šè·å–çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾ã€é¢„æµ‹åˆ†æ•°
            if self.opt.dataset  == 'semeval':
                aspect_P, aspect_R, aspect_F = semeval_PRF(y_true, y_pred)
                sentiment_Acc_4_classes = semeval_Acc(y_true, y_pred, score, 4)
                sentiment_Acc_3_classes = semeval_Acc(y_true, y_pred, score, 3)
                sentiment_Acc_2_classes = semeval_Acc(y_true, y_pred, score, 2)
                max_per = sentiment_Acc_4_classes    #ä»¥4ç±»æƒ…æ„Ÿå‡†ç¡®ç‡ä½œä¸ºæœ€ä½³æŒ‡æ ‡

                logger.info('')
                logger.info('>> P: {:.4f} , R: {:.4f} , F: {:.4f} '.format(aspect_P, aspect_R, aspect_F))
                logger.info('>> 2 classes acc: {:.4f} '.format(sentiment_Acc_2_classes))
                logger.info('>> 3 classes acc: {:.4f} '.format(sentiment_Acc_3_classes))
                logger.info('>> 4 classes acc: {:.4f} '.format(sentiment_Acc_4_classes))
            else:
                aspect_strict_Acc = sentihood_strict_acc(y_true, y_pred)
                aspect_Macro_F1 = sentihood_macro_F1(y_true, y_pred)
                aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC = sentihood_AUC_Acc(y_true, score)
                max_per = aspect_strict_Acc   #ä»¥ä¸¥æ ¼å‡†ç¡®ç‡ä½œä¸ºæœ€ä½³æŒ‡æ ‡

                logger.info('')
                logger.info('>> aspect_strict_Acc: {:.4f} , aspect_Macro_F1: {:.4f} , aspect_Macro_AUC: {:.4f} '.format(
                    aspect_strict_Acc,    
                    aspect_Macro_F1,
                    aspect_Macro_AUC
                ))
                logger.info('>> sentiment_Acc: {:.4f} '.format(sentiment_Acc))
                logger.info('>> sentiment_Macro_AUC: {:.4f} '.format(sentiment_Macro_AUC))


            # ------------------ Save best model ------------------            
            if max_per > max_val_f1:   #å½“å‰éªŒè¯é›†æ€§èƒ½ > å†å²æœ€ä½³
                max_val_f1 = max_per   #æ›´æ–°æœ€ä½³æŒ‡æ ‡
                best_epoch = epoch    #è®°å½•æœ€ä¼˜è½®æ¬¡
                if not os.path.exists('state_dict'):
                    os.mkdir('state_dict')
                path = copy.deepcopy(self.model.state_dict())   #æ·±æ‹·è´æ¨¡å‹å½“å‰å‚æ•°
                
    
            self.model.train()  
        logger.info(f"ğŸ”¥ Training Finished. Best Epoch = {best_epoch}")
        return path, best_epoch    #è¿”å›æœ€ä½³æ¨¡å‹çš„å‚æ•°å­—å…¸ã€æœ€ä¼˜è½®æ¬¡


        
    # ------------------------------ æ ¸å¿ƒè¯„ä¼°å‡½æ•° ------------------------------
    def _evaluate_acc_f1(self, data_loader):
        """æ ¸å¿ƒè¯„ä¼°å‡½æ•°ï¼šè·å–è¯„ä¼°æ•°æ®é›†çš„çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾ã€é¢„æµ‹åˆ†æ•°ï¼ˆç”¨äºåç»­è®¡ç®—å„ç±»æŒ‡æ ‡ï¼‰"""
        n_correct, n_total = 0, 0    #æ­£ç¡®é¢„æµ‹æ•°ã€æ€»æ ·æœ¬æ•°
        t_targets_all, t_outputs_all = None, None    #ç´¯è®¡æ‰€æœ‰æ‰¹æ¬¡çš„çœŸå®æ ‡ç­¾ã€æ¨¡å‹è¾“å‡ºlogits
        score = []   #å­˜å‚¨æ¯ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹åˆ†æ•°
        self.model.eval()
        with torch.no_grad():
            #éå†è¯„ä¼°æ•°æ®é›†çš„æ‰€æœ‰æ‰¹æ¬¡
            for t_batch, t_sample_batched in enumerate(data_loader):
                t_sample_batched = [b.to(self.opt.device) for b in t_sample_batched]
                input_ids, attention_mask, labels = t_sample_batched
                
                #æ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆé¢„æµ‹æ—¶ä¸ä¼ å…¥labelsï¼Œè¿”å›logitsï¼‰
                logits = self.model(input_ids, attention_mask)

                score.append(F.softmax(logits, dim=-1).detach().cpu().numpy())  #è®¡ç®—é¢„æµ‹åˆ†æ•°å¹¶æ·»åŠ åˆ°scoreåˆ—è¡¨

                n_correct += (torch.argmax(logits, -1) == labels).sum().item()
                n_total += len(logits)
                
                #æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡çš„çœŸå®æ ‡ç­¾å’Œlogits
                if t_targets_all is None:
                    t_targets_all = labels
                    t_outputs_all = logits
                else:
                    t_targets_all = torch.cat((t_targets_all, labels), dim=0)
                    t_outputs_all = torch.cat((t_outputs_all, logits), dim=0)
        #è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è¿”å›ï¼šçœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾ã€é¢„æµ‹åˆ†æ•°
        return t_targets_all.cpu().numpy(), torch.argmax(t_outputs_all, -1).cpu().numpy(), np.concatenate(score, axis=0)


    def make_weights_for_balanced_classes(self, labels, nclasses, fixed=False):
        """ ä¸ºä¸å¹³è¡¡æ•°æ®é›†ç”Ÿæˆæ ·æœ¬æƒé‡ï¼ˆç”¨äºWeightedRandomSamplerï¼Œè§£å†³ç±»åˆ«åˆ†å¸ƒä¸å‡é—®é¢˜ï¼‰"""
        if fixed:   #æ‰‹åŠ¨å›ºå®šæƒé‡æ¨¡å¼
            weight = [0] * len(labels)
            if nclasses == 3:   #3åˆ†ç±»åœºæ™¯ï¼šæ ‡ç­¾0æƒé‡0.2ï¼Œæ ‡ç­¾1å’Œ2æƒé‡0.4
                for idx, val in enumerate(labels):
                    if val == 0:
                        weight[idx] = 0.2
                    elif val == 1:
                        weight[idx] = 0.4
                    elif val == 2:
                        weight[idx] = 0.4
                return weight
            else:   #å…¶ä»–åˆ†ç±»åœºæ™¯ï¼šæ ‡ç­¾0æƒé‡0.2ï¼Œå…¶ä»–æ ‡ç­¾æƒé‡0.4
                for idx, val in enumerate(labels):
                    if val == 0:
                        weight[idx] = 0.2
                    else:
                        weight[idx] = 0.4
                return weight
        else:   #è‡ªåŠ¨è®¡ç®—æƒé‡æ¨¡å¼
            count = [0] * nclasses
            for item in labels:
                idx = int(item)   
                count[idx] += 1
            weight_per_class = [0.] * nclasses  
            N = float(sum(count))  
            
            #è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡ï¼šæ€»æ ·æœ¬æ•° / è¯¥ç±»æ ·æœ¬æ•°ï¼ˆé¢‘ç‡è¶Šé«˜ï¼Œæƒé‡è¶Šä½ï¼‰
            for i in range(nclasses):
                weight_per_class[i] = N / float(count[i])
            weight = [0] * len(labels)  
            for idx, val in enumerate(labels):
                weight[idx] = weight_per_class[val]  
            return weight


    
    # ======================================================
    # âœ… RUN = Train best + Test best
    # ======================================================
    def run(self):
        """
        è®­ç»ƒæµç¨‹ä¸»å‡½æ•°ï¼šæ•´åˆABSAä»»åŠ¡å…¨æµç¨‹ï¼ˆæ•°æ®é›†é¢„å¤„ç†â†’æ•°æ®åŠ è½½â†’ä¼˜åŒ–å™¨åˆå§‹åŒ–â†’è®­ç»ƒâ†’è¯„ä¼°â†’ä¿å­˜ï¼‰
        æ˜¯Instructorç±»çš„æ ¸å¿ƒå…¥å£ï¼Œè°ƒç”¨å…¶ä»–å‡½æ•°å®Œæˆç«¯åˆ°ç«¯è®­ç»ƒ
        """
        # 1. æå–è®­ç»ƒé›†æ‰€æœ‰æ ‡ç­¾å¹¶è½¬æ¢ä¸ºPyTorché•¿æ•´å‹å¼ é‡
        all_label_ids = torch.tensor([f['label'] for f in self.trainset], dtype=torch.long)   

        # 2. å°†è®­ç»ƒé›†è½¬æ¢ä¸ºTensorDatasetæ ¼å¼ï¼ŒåŒ…å«3ä¸ªå¼ é‡ï¼štokenç´¢å¼•ã€æ³¨æ„åŠ›æ©ç ã€æ ‡ç­¾å¼ é‡
        self.trainset = TensorDataset(
            torch.tensor([f['text_bert_indices'] for f in self.trainset], dtype=torch.long), 
            torch.tensor([f['input_mask'] for f in self.trainset], dtype=torch.long), 
            all_label_ids
        )
        
        # 3. ç”Ÿæˆè®­ç»ƒé›†æ ·æœ¬æƒé‡
        if self.opt.dataset == "semeval":
            sampler_weights = self.make_weights_for_balanced_classes(all_label_ids, 5)
        else:
            sampler_weights = self.make_weights_for_balanced_classes(all_label_ids, 3)

        # 4. åˆå§‹åŒ–åŠ æƒéšæœºé‡‡æ ·å™¨
        train_sampler = WeightedRandomSampler(sampler_weights, len(self.trainset), replacement=True)
        # 5. æ„å»ºè®­ç»ƒé›†æ•°æ®åŠ è½½å™¨
        train_data_loader= DataLoader(dataset=self.trainset, batch_size=self.opt.train_batch_size,sampler=train_sampler)

        # 6. è½¬æ¢æµ‹è¯•é›†ä¸ºTensorDatasetæ ¼å¼
        self.testset = TensorDataset(torch.tensor([f['text_bert_indices'] for f in self.testset], dtype=torch.long),
                                      torch.tensor([f['input_mask'] for f in self.testset], dtype=torch.long),
                                      torch.tensor([f['label'] for f in self.testset], dtype=torch.long))
        # 7. è½¬æ¢éªŒè¯é›†ä¸ºTensorDatasetæ ¼å¼
        self.valset = TensorDataset(torch.tensor([f['text_bert_indices'] for f in self.valset], dtype=torch.long),
                                     torch.tensor([f['input_mask'] for f in self.valset], dtype=torch.long),
                                     torch.tensor([f['label'] for f in self.valset], dtype=torch.long))
        
        # 8. æ„å»ºæµ‹è¯•é›†/éªŒè¯é›†æ•°æ®åŠ è½½å™¨
        test_data_loader = DataLoader(dataset=self.testset, batch_size=self.opt.eval_batch_size, shuffle=False)
        val_data_loader = DataLoader(dataset=self.valset, batch_size=self.opt.eval_batch_size, shuffle=False)

        # 9. è®¡ç®—æ€»è®­ç»ƒæ­¥æ•° = æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•°é‡ Ã— è®­ç»ƒè½®æ•°
        num_train_steps = int(len(train_data_loader) * self.opt.num_epoch)
        t_total = num_train_steps
        
        # 10. åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼šä¼ å…¥æ¨¡å‹å‚æ•°ã€å­¦ä¹ ç‡ã€L2æ­£åˆ™åŒ–ç³»æ•°
        optimizer= self.opt.optimizer(
            self.model.parameters(), 
            lr=self.opt.learning_rate,                        
            weight_decay=self.opt.l2reg
        )

        # 11. å¯åŠ¨è®­ç»ƒï¼šè°ƒç”¨_trainå‡½æ•°ï¼Œè¿”å›æœ€ä½³æ¨¡å‹çš„å‚æ•°å­—å…¸ï¼ˆstate_dictï¼‰
        best_model_path, best_epoch = self._train(optimizer, train_data_loader, val_data_loader, t_total)
        # 12. åŠ è½½æœ€ä½³æ¨¡å‹å‚æ•°ï¼ˆåŸºäºéªŒè¯é›†æ€§èƒ½çš„æœ€ä¼˜å‚æ•°ï¼Œç”¨äºæœ€ç»ˆæµ‹è¯•ï¼‰
        self.model.load_state_dict(best_model_path)


        # 13. æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°
        self.model.eval()
        #-------------------------- æµ‹è¯•é›†ä¸Šè¯„ä¼°å¹¶è¾“å‡ºæŒ‡æ ‡ ---------------------
        logger.info(f"ğŸ”¥ Testing Best Epoch = {best_epoch}")
        y_true, y_pred, score = self._evaluate_acc_f1(test_data_loader)     #åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œè·å–çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾å’Œåˆ†æ•°
        if self.opt.dataset=='semeval':
            aspect_P, aspect_R, aspect_F = semeval_PRF(y_true, y_pred)
            sentiment_Acc_4_classes = semeval_Acc(y_true, y_pred, score, 4)
            sentiment_Acc_3_classes = semeval_Acc(y_true, y_pred, score, 3)
            sentiment_Acc_2_classes = semeval_Acc(y_true, y_pred, score, 2)
            
            logger.info('>> P: {:.4f} , R: {:.4f} , F: {:.4f} '.format(aspect_P, aspect_R, aspect_F))
            logger.info('>> 4 classes acc: {:.4f} '.format(sentiment_Acc_4_classes))
            logger.info('>> 3 classes acc: {:.4f} '.format(sentiment_Acc_3_classes))
            logger.info('>> 2 classes acc: {:.4f} '.format(sentiment_Acc_2_classes))
        else:
            aspect_strict_Acc = sentihood_strict_acc(y_true, y_pred)
            aspect_Macro_F1 = sentihood_macro_F1(y_true, y_pred)
            aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC = sentihood_AUC_Acc(y_true, score)
            
            logger.info('>> aspect_strict_Acc: {:.4f} , aspect_Macro_F1: {:.4f} , aspect_Macro_AUC: {:.4f} '.format(
                    aspect_strict_Acc,    
                    aspect_Macro_F1,
                    aspect_Macro_AUC
            ))
            logger.info('>> sentiment_Acc: {:.4f} '.format(sentiment_Acc))
            logger.info('>> sentiment_Macro_AUC: {:.4f} '.format(sentiment_Macro_AUC))

            
        # ------------------------  æŒ‰ seed ä¿å­˜æ¨¡å‹ ----------------------------------
        # 14. ä¿å­˜æœ€ä½³æ¨¡å‹å‚æ•°
        if self.opt.save_model:
            save_dir = f"state_dict/{self.opt.dataset}"
            os.makedirs(save_dir, exist_ok=True)
            #æ„å»ºæ¨¡å‹ä¿å­˜è·¯å¾„ï¼šstate_dict/æ•°æ®é›†å/seed.bm
            save_path = f"{save_dir}/seed{self.opt.seed}.bm"
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            torch.save(self.model.state_dict(), save_path)   #ä¿å­˜æ¨¡å‹å‚æ•°å­—å…¸
            logger.info(f"ğŸ’¾ Model saved to {save_path}")



def main():
    """ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°ã€è®¾ç½®éšæœºç§å­ã€é…ç½®æ•°æ®é›†è·¯å¾„ã€åˆå§‹åŒ–è®­ç»ƒæŒ‡å¯¼ç±»å¹¶å¯åŠ¨è®­ç»ƒ"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', default='semeval',  choices=['semeval','sentihood'], type=str, required=True)
    parser.add_argument('--learning_rate', default=3e-5, type=float, help='try 5e-5, 2e-5')
    parser.add_argument('--dropout', default=0.1, type=float)
    parser.add_argument('--l2reg', default=0.001, type=float)
    parser.add_argument('--warmup_proportion', default=0.01, type=float)
    parser.add_argument('--num_epoch', default=5, type=int, help='')
    parser.add_argument("--train_batch_size", default=32,type=int, help="Total batch size for training.")
    parser.add_argument("--eval_batch_size", default=64, type=int, help="Total batch size for eval.")
    parser.add_argument('--log_step', default=50, type=int)
    parser.add_argument('--max_seq_len', default=120, type=int)
    parser.add_argument('--label_dim', default=5, type=int)
    parser.add_argument('--pretrained_model', default='bart-base', type=str) 
    parser.add_argument('--save_model', default=0, type=int)
    parser.add_argument('--device', default='cuda', type=str, help='e.g. cuda:0')
    parser.add_argument('--seed', default=42, type=int, help='set seed for reproducibility')
    parser.add_argument('--valset_ratio', default=0, type=float,
                        help='set ratio between 0 and 1 for validation support')
    opt = parser.parse_args()


    if opt.dataset=='sentihood':
        opt.label_dim =3

        
    #è®¾ç½®éšæœºç§å­ï¼ˆä¿è¯ç»“æœå¯å¤ç°ï¼‰
    if opt.seed is not None:
        random.seed(opt.seed)
        numpy.random.seed(opt.seed)
        torch.manual_seed(opt.seed)
        torch.cuda.manual_seed(opt.seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

        
    #é…ç½®æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆè®­ç»ƒé›†ã€æµ‹è¯•é›†ã€éªŒè¯é›†ï¼‰
    dataset_files = {
        'train': '../../../datasets/{}/bert_train.json'.format(opt.dataset),
        'test': '../../../datasets/{}/bert_test.json'.format(opt.dataset),
        'val': '../../../datasets/{}/bert_dev.json'.format(opt.dataset)
    }

    
    logger.info(opt.pretrained_model) 
    opt.optimizer = AdamW
    opt.model_class = ABSATokenizer
    opt.dataset_file = dataset_files
    opt.inputs_cols = ['text_bert_indices', 'input_mask', 'label']   #é…ç½®æ¨¡å‹è¾“å…¥åˆ—åï¼ˆä¸æ•°æ®é›†å¼ é‡å¯¹åº”ï¼‰
    opt.initializer = torch.nn.init.xavier_uniform_
    opt.device = torch.device(opt.device if torch.cuda.is_available() else 'cpu')

    ins = Instructor(opt)
    ins.run()


if __name__ == '__main__':
    main()
